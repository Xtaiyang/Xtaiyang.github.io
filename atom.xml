<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Sunshine</title>
  
  
  <link href="https://xtaiyang.github.io/atom.xml" rel="self"/>
  
  <link href="https://xtaiyang.github.io/"/>
  <updated>2025-03-11T08:14:12.507Z</updated>
  <id>https://xtaiyang.github.io/</id>
  
  <author>
    <name>Sun</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>DeepSeek+LoRA+FastAPI-微调大模型并暴露接口给后端调用</title>
    <link href="https://xtaiyang.github.io/posts/5a2f1b49.html"/>
    <id>https://xtaiyang.github.io/posts/5a2f1b49.html</id>
    <published>2025-03-11T08:04:47.000Z</published>
    <updated>2025-03-11T08:14:12.507Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、简介"><a href="#一、简介" class="headerlink" title="一、简介"></a>一、简介</h2><p>本期视频主要分为以下五部分：</p><h5 id="1-需求和技术"><a href="#1-需求和技术" class="headerlink" title="1. 需求和技术"></a>1. 需求和技术</h5><ul><li>企业对于大模型的<strong>不同类型</strong>个性化需求</li><li><strong>SFT</strong>（有监督微调）、<strong>RLHF</strong>（强化学习）、<strong>RAG</strong>（检索增强生成）<br>-关注：基本概念；分别解决什么问题；如何根据需求选择；</li><li><strong>微调</strong>部分详细介绍：<ul><li>微调算法的分类</li><li>**LoRA 微调算法</li><li>微调常见实现框架</li></ul></li></ul><h5 id="2-整体步骤说明"><a href="#2-整体步骤说明" class="headerlink" title="2. 整体步骤说明"></a>2. 整体步骤说明</h5><ul><li>在 Linux 系统上微调一个大模型、部署模型、暴露 API 给 web 后端调用，本机前端展示全过程</li></ul><h5 id="3-模型微调"><a href="#3-模型微调" class="headerlink" title="3. 模型微调"></a>3. 模型微调</h5><ul><li>框架: <strong>LLama-Factory</strong> (国产最热门的微调框架)</li><li>算法: **LoRA (最著名的部分参数微调算法）</li><li>基座模型：<strong>DeepSeek-R1-Distill-Qwen-1.5B</strong><br>-蒸馏技术通常用于通过将大模型（教师模型）的知识转移到小模型（学生模型）中，使得小模型能够在尽量保持性能的同时，显著减少模型的参数量和计算需求。</li></ul><h5 id="4-模型部署和暴露接口"><a href="#4-模型部署和暴露接口" class="headerlink" title="4. 模型部署和暴露接口"></a>4. 模型部署和暴露接口</h5><ul><li>框架：<strong>FastAPI</strong>（一个基于 python 的 web 框架）</li></ul><h5 id="5-web后端调用"><a href="#5-web后端调用" class="headerlink" title="5. web后端调用"></a>5. web后端调用</h5><ul><li>通过 <strong>HTTP</strong> 请求交互即可（ Demo 前后端代码都在视频简介）</li></ul><h2 id="二、需求和技术"><a href="#二、需求和技术" class="headerlink" title="二、需求和技术"></a>二、需求和技术</h2><h5 id="1-企业对于大模型的不同类型个性化需求"><a href="#1-企业对于大模型的不同类型个性化需求" class="headerlink" title="1. 企业对于大模型的不同类型个性化需求"></a>1. 企业对于大模型的不同类型个性化需求</h5><ul><li>提高模型对<strong>企业专有信息</strong>的理解、增强模型在<strong>特定行业领域</strong>的知识 - <strong>SFT</strong><ul><li>案例一：希望大模型能更好理解蟹堡王的企业专有知识，如蟹老板的女儿为什么是一头鲸鱼</li><li>案例二：希望大模型能特别精通于汉堡制作，并熟练回答关于汉堡行业的所有问题</li></ul></li><li>提供<strong>个性化和互动性强</strong>的服务 - <strong>RLHF</strong><ul><li>案例三：希望大模型能够基于顾客的反馈调整回答方式，比如生成更二次元风格的回答还是更加学术风格的回答</li></ul></li><li>提高模型对企业专有信息的理解、增强模型在特定行业领域的知识、<strong>获取和生成最新的、实时的信息</strong> - <strong>RAG</strong><ul><li>案例四：希望大模型能够实时获取蟹堡王的最新的促销活动信息和每周菜单更新</li></ul></li></ul><h5 id="2-SFT（有监督微调）、RLHF（强化学习）、RAG（检索增强生成）"><a href="#2-SFT（有监督微调）、RLHF（强化学习）、RAG（检索增强生成）" class="headerlink" title="2. SFT（有监督微调）、RLHF（强化学习）、RAG（检索增强生成）"></a>2. SFT（有监督微调）、RLHF（强化学习）、RAG（检索增强生成）</h5><h6 id="2-1-SFT（Supervised-Fine-Tuning）有监督微调"><a href="#2-1-SFT（Supervised-Fine-Tuning）有监督微调" class="headerlink" title="2.1 SFT（Supervised Fine-Tuning）有监督微调"></a>2.1 SFT（Supervised Fine-Tuning）有监督微调</h6><ul><li>通过提供人工标注的数据，进一步训练<strong>预训练模型</strong>，让模型能够更加精准地处理特定领域的任务</li><li>除了“有监督微调”，还有“无监督微调”“自监督微调”，当大家提到“微调”时通常是指有监督微调</li></ul><h6 id="2-2-RLHF（Reinforcement-Learning-from-Human-Feedback）强化学习"><a href="#2-2-RLHF（Reinforcement-Learning-from-Human-Feedback）强化学习" class="headerlink" title="2.2 RLHF（Reinforcement Learning from Human Feedback）强化学习"></a>2.2 RLHF（Reinforcement Learning from Human Feedback）强化学习</h6><ul><li>DPO（Direct Preference Optimization）<br>核心思想：通过 <strong>人类对比选择</strong>（例如：A 选项和 B 选项，哪个更好）直接优化生成模型，使其产生更符合用户需求的结果；调整幅度大<br><img src="https://img.xtaiyang.store/windows/20250311160609563.png" alt="Pasted image 20250224205057.png"></li><li>PPO（Proximal Policy Optimization）<br>核心思想：通过 <strong>奖励信号</strong>（如点赞、点踩）来 <strong>渐进式调整模型的行为策略</strong>；调整幅度小<br><img src="https://img.xtaiyang.store/windows/20250311160836273.png" alt="截屏2025-02-24 20.53.22.png"></li></ul><h6 id="2-3-RAG（Retrieval-Augmented-Generation）检索增强生成"><a href="#2-3-RAG（Retrieval-Augmented-Generation）检索增强生成" class="headerlink" title="2.3 RAG（Retrieval-Augmented Generation）检索增强生成"></a>2.3 RAG（Retrieval-Augmented Generation）检索增强生成</h6><ul><li>将外部信息检索与文本生成结合，帮助模型在生成答案时，实时获取外部信息和最新信息</li></ul><h5 id="3-微调还是RAG"><a href="#3-微调还是RAG" class="headerlink" title="3. 微调还是RAG?"></a>3. 微调还是RAG?</h5><ul><li>微调：<ul><li>适合：拥有非常充足的数据</li><li>能够直接提升模型的固有能力；无需依赖外部检索；</li></ul></li><li>RAG:<ul><li>适合：只有非常非常少的数据；动态更新的数据</li><li>每次回答问题前需耗时检索知识库；回答质量依赖于检索系统的质量；</li></ul></li><li>总结：<ul><li>少量企业私有知识：最好微调和 RAG 都做；资源不足时优先 RAG；</li><li>会动态更新的知识：RAG</li><li>大量垂直领域知识：微调</li></ul></li></ul><h5 id="4-SFT（有监督微调）"><a href="#4-SFT（有监督微调）" class="headerlink" title="4. SFT（有监督微调）"></a>4. SFT（有监督微调）</h5><p> 通过提供<strong>人工标注</strong>的数据，进一步训练<strong>预训练模型</strong>，让模型能够更加精准地处理<strong>特定领域</strong>的任务</p><ul><li>人工标注的数据</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">如：分类系统</span><br><span class="line">&#123;&quot;image_path&quot;: &quot;path/image1.jpg&quot;, &quot;label&quot;: &quot;SpongeBobSquarePants&quot;&#125;</span><br><span class="line">&#123;&quot;image_path&quot;: &quot;path/image2.jpg&quot;, &quot;label&quot;: &quot;PatrickStar&quot;&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">如：对话系统</span><br><span class="line">&#123;</span><br><span class="line">    &quot;instruction&quot;: &quot;请问你是谁&quot;,</span><br><span class="line">    &quot;input&quot;: &quot;&quot;,</span><br><span class="line">    &quot;output&quot;: &quot;您好，我是蟹堡王的神奇海螺，很高兴为您服务！我可以回答关于蟹堡王和汉堡制作的任何问题，您有什么需要帮助的吗？&quot;</span><br><span class="line">&#125;,</span><br></pre></td></tr></table></figure><ul><li>预训练模型（基座模型）<br>指已经在大量数据上训练过的模型，也就是我们微调前需要预先下载的开源模型。它具备了较为通用的知识和能力，能够解决一些常见的任务，可以在此基础上进行进一步的微调（fine-tuning）以适应特定的任务或领域</li><li>微调算法的分类<ul><li><strong>全参数微调（Full Fine-Tuning）</strong>：<ul><li>对整个预训练模型进行微调，会更新所有参数。<ul><li>优点：因为每个参数都可以调整，通常能得到最佳的性能；能够适应不同任务和场景</li><li>缺点：需要较大的计算资源并且容易出现过拟合</li></ul></li></ul></li><li><strong>部分参数微调（Partial Fine-Tuning）</strong>：<ul><li>只更新模型的部分参数（例如某些层或模块）</li><li>优点：减少了计算成本；减少过拟合风险；能够以较小的代价获得较好的结果</li><li>缺点：可能无法达到最佳性能</li><li>最著名算法：LoRA</li></ul></li></ul></li></ul><h5 id="5-LoRA-微调算法"><a href="#5-LoRA-微调算法" class="headerlink" title="5. LoRA  微调算法"></a>5. LoRA  微调算法</h5><ul><li>论文阅读：<ul><li>LoRA 开山论文：2021 年 Microsoft Research 提出，首次提出了通过<strong>低秩矩阵分解</strong>的方式来进行<strong>部分参数微调</strong>，极大推动了 AI 技术在多行业的广泛落地应用：<a href="https://arxiv.org/abs/2106.09685">LoRA: Low-Rank Adaptation of Large Language Models</a></li><li>大语言模型开山论文：2017 年 Google Brain 团队发布，标志着 <strong>Transformer</strong> 架构的提出，彻底改变了自然语言处理（NLP）领域，标志着大语言模型时代的开始：<a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a></li></ul></li><li>什么是矩阵的“秩”<ul><li>矩阵的秩（Rank of a matrix）是指矩阵中<strong>线性无关</strong>的行或列的最大数量。简单来说它能反映矩阵所包含的<strong>有效信息量</strong></li></ul></li><li>LoRA 如何做到部分参数微调<br><img src="https://img.xtaiyang.store/windows/20250311160928147.png" alt="Pasted image 20250225121007.png"></li><li>LoRA 训练结束后通常需要进行权重合并</li></ul><h5 id="6-微调常见实现框架"><a href="#6-微调常见实现框架" class="headerlink" title="6. 微调常见实现框架"></a>6. 微调常见实现框架</h5><ul><li><a href="https://www.zhihu.com/question/638803488/answer/84354509523">初学者如何对大模型进行微调？</a></li><li><strong>Llama-Factory</strong>：由国内<strong>北航</strong>开源的低代码大模型训练框架，可以实现<strong>零代码微调</strong>，简单易学，功能强大，且目前热度很高，建议新手从这个开始入门</li><li><strong>transformers.Trainer</strong>：由 <strong>Hugging Face</strong> 提供的高层 <strong>API</strong>，适用于各种 NLP 任务的微调，提供标准化的训练流程和多种监控工具，适合需要更多<strong>定制化</strong>的场景，尤其在<strong>部署和生产环境</strong>中表现出色</li><li><strong>DeepSpeed</strong>：由<strong>微软</strong>开发的开源深度学习优化库，适合大规模模型训练和<strong>分布式训练</strong>，在大模型<strong>预训练</strong>和资源密集型训练的时候用得比较多</li></ul><h2 id="三、整体步骤说明"><a href="#三、整体步骤说明" class="headerlink" title="三、整体步骤说明"></a>三、整体步骤说明</h2><h2 id="四、模型微调"><a href="#四、模型微调" class="headerlink" title="四、模型微调"></a>四、模型微调</h2><h5 id="1-准备硬件资源、搭建环境"><a href="#1-准备硬件资源、搭建环境" class="headerlink" title="1. 准备硬件资源、搭建环境"></a>1. 准备硬件资源、搭建环境</h5><ul><li>在云平台上租用一个实例（如 <strong>AutoDL</strong>，官网：<a href="https://www.autodl.com/market/list">https://www.autodl.com/market/list</a>）</li><li>云平台一般会配置好常用的深度学习环境，如 anaconda, cuda等等</li></ul><h5 id="2-本机通过-SSH-连接到远程服务器"><a href="#2-本机通过-SSH-连接到远程服务器" class="headerlink" title="2. 本机通过 SSH 连接到远程服务器"></a>2. 本机通过 SSH 连接到远程服务器</h5><ul><li>使用 Visual Studio Remote 插件 SSH 连接到你租用的服务器，参考文档: <a href="https://www.cnblogs.com/qiuhlee/p/17729647.html"># 使用VSCode插件Remote-SSH连接服务器</a></li><li>连接后打开个人数据盘文件夹 <strong>/root/autodl-tmp</strong></li></ul><h5 id="3-LLaMA-Factory-安装部署"><a href="#3-LLaMA-Factory-安装部署" class="headerlink" title="3. LLaMA-Factory 安装部署"></a>3. LLaMA-Factory 安装部署</h5><p>LLaMA-Factory 的 Github地址：<a href="https://github.com/hiyouga/LLaMA-Factory">https://github.com/hiyouga/LLaMA-Factory</a></p><ul><li>克隆仓库</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> --depth 1 https://github.com/hiyouga/LLaMA-Factory.git</span><br></pre></td></tr></table></figure><ul><li>切换到项目目录</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> LLaMA-Factory</span><br></pre></td></tr></table></figure><ul><li>修改配置，将 conda 虚拟环境安装到数据盘（这一步也可不做）</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> -p /root/autodl-tmp/conda/pkgs </span><br><span class="line">conda config --add pkgs_dirs /root/autodl-tmp/conda/pkgs </span><br><span class="line"><span class="built_in">mkdir</span> -p /root/autodl-tmp/conda/envs </span><br><span class="line">conda config --add envs_dirs /root/autodl-tmp/conda/envs</span><br></pre></td></tr></table></figure><ul><li>创建 conda 虚拟环境(一定要 3.10 的 python 版本，不然和 LLaMA-Factory 不兼容)</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create -n llama-factory python=3.10</span><br></pre></td></tr></table></figure><ul><li>激活虚拟环境</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda activate llama-factory</span><br></pre></td></tr></table></figure><ul><li>在虚拟环境中安装 LLaMA Factory 相关依赖</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -e <span class="string">&quot;.[torch,metrics]&quot;</span></span><br></pre></td></tr></table></figure><pre><code>注意：如报错 bash: pip: command not found ，先执行 conda install pip 即可</code></pre><ul><li>检验是否安装成功</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">llamafactory-cli version</span><br></pre></td></tr></table></figure><h5 id="4-启动-LLama-Factory-的可视化微调界面-（由-Gradio-驱动）"><a href="#4-启动-LLama-Factory-的可视化微调界面-（由-Gradio-驱动）" class="headerlink" title="4. 启动 LLama-Factory 的可视化微调界面 （由 Gradio 驱动）"></a>4. 启动 LLama-Factory 的可视化微调界面 （由 Gradio 驱动）</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">llamafactory-cli webui</span><br></pre></td></tr></table></figure><h5 id="5-配置端口转发"><a href="#5-配置端口转发" class="headerlink" title="5. 配置端口转发"></a>5. 配置端口转发</h5><ul><li>参考文档：<a href="https://www.autodl.com/docs/ssh_proxy/">SSH隧道</a></li><li>在<strong>本地电脑</strong>的终端(cmd / powershell / terminal等)中执行代理命令，其中<code>root@123.125.240.150</code>和<code>42151</code>分别是实例中SSH指令的访问地址与端口，请找到自己实例的ssh指令做相应<strong>替换</strong>。<code>7860:127.0.0.1:7860</code>是指代理实例内<code>7860</code>端口到本地的<code>7860</code>端口</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh -CNg -L 7860:127.0.0.1:7860 root@123.125.240.150 -p 42151</span><br></pre></td></tr></table></figure><h5 id="6-从-HuggingFace-上下载基座模型"><a href="#6-从-HuggingFace-上下载基座模型" class="headerlink" title="6. 从 HuggingFace 上下载基座模型"></a>6. 从 HuggingFace 上下载基座模型</h5><p>HuggingFace 是一个集中管理和共享预训练模型的平台  <a href="https://huggingface.co">https://huggingface.co</a>;<br>从 HuggingFace 上下载模型有多种不同的方式，可以参考：<a href="https://zhuanlan.zhihu.com/p/663712983">如何快速下载huggingface模型——全方法总结</a></p><ul><li>创建文件夹统一存放所有基座模型</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> Hugging-Face</span><br></pre></td></tr></table></figure><ul><li>修改 HuggingFace 的镜像源 </li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> HF_ENDPOINT=https://hf-mirror.com</span><br></pre></td></tr></table></figure><ul><li>修改模型下载的默认位置</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> HF_HOME=/root/autodl-tmp/Hugging-Face</span><br></pre></td></tr></table></figure><ul><li>注意：这种配置方式只在当前 shell 会话中有效，如果你希望这个环境变量在每次启动终端时都生效，可以将其添加到你的用户配置文件中（修改 <code>~/.bashrc</code> 或 <code>~/.zshrc</code>）</li><li>检查环境变量是否生效</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="variable">$HF_ENDPOINT</span></span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$HF_HOME</span></span><br></pre></td></tr></table></figure><ul><li>安装 HuggingFace 官方下载工具</li></ul><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -U huggingface_hub</span><br></pre></td></tr></table></figure><ul><li>执行下载命令</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">huggingface-cli download --resume-download deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B</span><br></pre></td></tr></table></figure><ul><li>如果直接本机下载了模型压缩包，如何放到你的服务器上？——在 AutoDL 上打开 JupyterLab 直接上传，或者下载软件通过 SFTP 协议传送</li></ul><h5 id="7-可视化页面上加载模型测试，检验是否加载成功"><a href="#7-可视化页面上加载模型测试，检验是否加载成功" class="headerlink" title="7. 可视化页面上加载模型测试，检验是否加载成功"></a>7. 可视化页面上加载模型测试，检验是否加载成功</h5><ul><li>注意：这里的路径是模型文件夹内部的<strong>模型特定快照的唯一哈希值</strong>，而不是整个模型文件夹<br><img src="https://img.xtaiyang.store/windows/20250311161034588.png" alt="1.png"></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/root/autodl-tmp/Hugging-Face/hub/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-1.5B/snapshots/530ca3e1ad39d440e182c2e4317aa40f012512fa</span><br></pre></td></tr></table></figure><h5 id="8-准备用于训练的数据集，添加到指定位置"><a href="#8-准备用于训练的数据集，添加到指定位置" class="headerlink" title="8. 准备用于训练的数据集，添加到指定位置"></a>8. 准备用于训练的数据集，添加到指定位置</h5><ul><li><strong>README_zh</strong> 中详细介绍了如何配置和描述你的自定义数据集</li><li>按照格式准备用于微调的数据集 <strong>magic_conch.json</strong>，数据示例：</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[</span><br><span class="line">  &#123;</span><br><span class="line">    &quot;instruction&quot;: &quot;请问你是谁&quot;,</span><br><span class="line">    &quot;input&quot;: &quot;&quot;,</span><br><span class="line">    &quot;output&quot;: &quot;您好，我是蟹堡王的神奇海螺，很高兴为您服务！我可以回答关于蟹堡王和汉堡制作的任何问题，您有什么需要帮助的吗？&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &#123;</span><br><span class="line">    &quot;instruction&quot;: &quot;怎么修复这个报错&quot;,</span><br><span class="line">    &quot;input&quot;: &quot;我正在使用蟹堡王全自动智能汉堡制作机，报错信息是：汉堡食谱为空&quot;,</span><br><span class="line">    &quot;output&quot;: &quot;根据您提供的错误信息，&#x27;汉堡食谱为空&#x27; 可能是因为系统没有加载正确的食谱文件或食谱文件被删除。您可以尝试以下步骤：\n1. 检查食谱文件是否存在，并确保文件路径正确。\n2. 重新加载或更新食谱文件。\n3. 如果问题依然存在，尝试重启机器并检查是否有软件更新。\n希望这些步骤能帮助您修复问题。如果仍有困难，请与蟹堡王技术支持联系。&quot;</span><br><span class="line">  &#125;</span><br><span class="line">]</span><br></pre></td></tr></table></figure><ul><li>修改 <strong>dataset_info.json</strong> 文件，添加如下配置：</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&quot;magic_conch&quot;: &#123;</span><br><span class="line">&quot;file_name&quot;: &quot;magic_conch.json&quot;</span><br><span class="line">&#125;,</span><br></pre></td></tr></table></figure><ul><li>将数据集 magic_conch.json 放到 LLama-Factory 的 <strong>data 目录</strong> 下</li></ul><h5 id="9-在页面上进行微调的相关设置，开始微调"><a href="#9-在页面上进行微调的相关设置，开始微调" class="headerlink" title="9. 在页面上进行微调的相关设置，开始微调"></a>9. 在页面上进行微调的相关设置，开始微调</h5><ul><li>选择微调算法 <strong>Lora</strong></li><li>添加数据集 <strong>magic_conch</strong></li><li>修改其他训练相关参数，如学习率、训练轮数、截断长度、验证集比例等<ul><li>学习率（Learning Rate）：决定了模型每次更新时权重改变的幅度。过大可能会错过最优解；过小会学得很慢或陷入局部最优解</li><li>训练轮数（Epochs）：太少模型会欠拟合（没学好），太大会过拟合（学过头了）</li><li>最大梯度范数（Max Gradient Norm）：当梯度的值超过这个范围时会被截断，防止梯度爆炸现象</li><li>最大样本数（Max Samples）：每轮训练中最多使用的样本数</li><li>计算类型（Computation Type）：在训练时使用的数据类型，常见的有 float32 和 float16。在性能和精度之间找平衡</li><li>截断长度（Truncation Length）：处理长文本时如果太长超过这个阈值的部分会被截断掉，避免内存溢出</li><li>批处理大小（Batch Size）：由于内存限制，每轮训练我们要将训练集数据分批次送进去，这个批次大小就是 Batch Size</li><li>梯度累积（Gradient Accumulation）：默认情况下模型会在每个 batch 处理完后进行一次更新一个参数，但你可以通过设置这个梯度累计，让他直到处理完多个小批次的数据后才进行一次更新</li><li>验证集比例（Validation Set Proportion）：数据集分为训练集和验证集两个部分，训练集用来学习训练，验证集用来验证学习效果如何</li><li>学习率调节器（Learning Rate Scheduler）：在训练的过程中帮你自动调整优化学习率</li></ul></li><li>页面上点击<strong>启动训练</strong>，或复制命令到终端启动训练<ul><li>实践中推荐用 <code>nohup</code> 命令将训练任务放到后台执行，这样即使关闭终端任务也会继续运行。同时将日志重定向到文件中保存下来</li></ul></li><li>在训练过程中注意观察损失曲线，<strong>尽可能将损失降到最低</strong><ul><li>如损失降低太慢，尝试增大学习率</li><li>如训练结束损失还呈下降趋势，增大训练轮数确保拟合</li></ul></li></ul><h5 id="10-微调结束，评估微调效果"><a href="#10-微调结束，评估微调效果" class="headerlink" title="10. 微调结束，评估微调效果"></a>10. 微调结束，评估微调效果</h5><ul><li>观察损失曲线的变化；观察最终损失</li><li>在交互页面上通过预测/对话等方式测试微调好的效果</li><li><strong>检查点</strong>：保存的是模型在训练过程中的一个中间状态，包含了模型权重、训练过程中使用的配置（如学习率、批次大小）等信息，对LoRA来说，检查点包含了<strong>训练得到的 B 和 A 这两个低秩矩阵的权重</strong></li><li>若微调效果不理想，你可以：<ul><li>使用更强的预训练模型</li><li>增加数据量</li><li>优化数据质量（数据清洗、数据增强等，可学习相关论文如何实现）</li><li>调整训练参数，如学习率、训练轮数、优化器、批次大小等等</li></ul></li></ul><h5 id="11-导出合并后的模型"><a href="#11-导出合并后的模型" class="headerlink" title="11. 导出合并后的模型"></a>11. 导出合并后的模型</h5><ul><li>为什么要合并：因为 LoRA 只是通过<strong>低秩矩阵</strong>调整原始模型的部分权重，而<strong>不直接修改原模型的权重</strong>。合并步骤将 LoRA 权重与原始模型权重融合生成一个完整的模型</li><li>先创建目录，用于存放导出后的模型</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p Models/deepseek-r1-1.5b-merged</span><br></pre></td></tr></table></figure><ul><li>在页面上配置导出路径，导出即可<br><img src="https://img.xtaiyang.store/windows/20250311161102399.png" alt="截屏2025-02-23 21.09.52.png"></li></ul><h2 id="五、模型部署和暴露接口"><a href="#五、模型部署和暴露接口" class="headerlink" title="五、模型部署和暴露接口"></a>五、模型部署和暴露接口</h2><h5 id="1-创建新的-conda-虚拟环境用于部署模型"><a href="#1-创建新的-conda-虚拟环境用于部署模型" class="headerlink" title="1. 创建新的 conda 虚拟环境用于部署模型"></a>1. 创建新的 conda 虚拟环境用于部署模型</h5><ul><li>创建环境</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create -n fastApi python=3.10</span><br></pre></td></tr></table></figure><ul><li>激活环境</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda activate fastApi</span><br></pre></td></tr></table></figure><ul><li>在该环境中下载部署模型需要的依赖</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install -c conda-forge fastapi uvicorn transformers pytorch</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install safetensors sentencepiece protobuf</span><br></pre></td></tr></table></figure><h5 id="2-通过-FastAPI-部署模型并暴露-HTTP-接口"><a href="#2-通过-FastAPI-部署模型并暴露-HTTP-接口" class="headerlink" title="2. 通过 FastAPI 部署模型并暴露 HTTP 接口"></a>2. 通过 FastAPI 部署模型并暴露 HTTP 接口</h5><ul><li>创建 App 文件夹</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> App</span><br></pre></td></tr></table></figure><ul><li>创建 main.py 文件，作为启动应用的入口</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">touch</span> main.py</span><br></pre></td></tr></table></figure><ul><li>修改 main.py 文件并保存</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> fastapi <span class="keyword">import</span> FastAPI</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForCausalLM, AutoTokenizer</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">app = FastAPI()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型路径</span></span><br><span class="line">model_path = <span class="string">&quot;/root/autodl-tmp/Models/deepseek-r1-1.5b-merged&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载 tokenizer （分词器）</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_path)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载模型并移动到可用设备（GPU/CPU）</span></span><br><span class="line">device = <span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span></span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(model_path).to(device)</span><br><span class="line"></span><br><span class="line"><span class="meta">@app.get(<span class="params"><span class="string">&quot;/generate&quot;</span></span>)</span></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">generate_text</span>(<span class="params">prompt: <span class="built_in">str</span></span>):</span><br><span class="line">    <span class="comment"># 使用 tokenizer 编码输入的 prompt</span></span><br><span class="line">    inputs = tokenizer(prompt, return_tensors=<span class="string">&quot;pt&quot;</span>).to(device)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 使用模型生成文本</span></span><br><span class="line">    outputs = model.generate(inputs[<span class="string">&quot;input_ids&quot;</span>], max_length=<span class="number">150</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 解码生成的输出</span></span><br><span class="line">    generated_text = tokenizer.decode(outputs[<span class="number">0</span>], skip_special_tokens=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&quot;generated_text&quot;</span>: generated_text&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><ul><li>进入包含 <code>main.py</code> 文件的目录，然后运行以下命令来启动 FastAPI 应用</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">uvicorn main:app --reload --host 0.0.0.0</span><br></pre></td></tr></table></figure><pre><code>- `main` 是 Python 文件名（要注意不包含 `.py` 扩展名）- `app` 是 FastAPI 实例的变量名（代码中 `app = FastAPI()`）- `--reload` 使代码更改后可以自动重载，适用于开发环境- `host 0.0.0.0`：将 FastAPI 应用绑定到所有可用的网络接口，这样我们的本机就可以通过内网穿透访问该服务</code></pre><ul><li>配置端口转发，使得本机可以访问该服务 <a href="https://www.autodl.com/docs/ssh_proxy/">SSH隧道</a></li><li>浏览器输入以下 url，测试服务是否启动成功</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://localhost:8000/docs</span><br></pre></td></tr></table></figure><p><img src="https://img.xtaiyang.store/windows/20250311161138539.png" alt="Pasted image 20250223232930.png"></p><ul><li>或者你也可以通过 postMan 来测试</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://localhost:8000/generate?prompt=你是谁？</span><br></pre></td></tr></table></figure><p><img src="https://img.xtaiyang.store/windows/20250311161208238.png" alt="Pasted image 20250223232958.png"></p><h2 id="六、web后端调用"><a href="#六、web后端调用" class="headerlink" title="六、web后端调用"></a>六、web后端调用</h2><h5 id="1-pom-xml-导入依赖"><a href="#1-pom-xml-导入依赖" class="headerlink" title="1. pom.xml 导入依赖"></a>1. pom.xml 导入依赖</h5><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span>  </span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.httpcomponents.client5<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span>  </span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>httpclient5<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span>  </span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>5.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span>  </span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h5 id="2-自定义方法发送并处理-HTTP-请求，实现对话功能"><a href="#2-自定义方法发送并处理-HTTP-请求，实现对话功能" class="headerlink" title="2. 自定义方法发送并处理 HTTP 请求，实现对话功能"></a>2. 自定义方法发送并处理 HTTP 请求，实现对话功能</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Service</span>  </span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ChatServiceImpl</span> <span class="keyword">implements</span> <span class="title class_">ChatService</span> &#123;  </span><br><span class="line">      </span><br><span class="line">    <span class="meta">@Autowired</span>  </span><br><span class="line">    <span class="keyword">private</span> RestTemplate restTemplate;  </span><br><span class="line">    <span class="meta">@Autowired</span>  </span><br><span class="line">    <span class="keyword">private</span> AiServiceConfig aiServiceConfig;  </span><br><span class="line">  </span><br><span class="line">    <span class="meta">@Override</span>  </span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">callAiForOneReply</span><span class="params">(String prompt)</span> &#123;  </span><br><span class="line">        <span class="comment">// 获取基础URL http://localhost:8000  </span></span><br><span class="line">        <span class="type">String</span> <span class="variable">baseUrl</span> <span class="operator">=</span> aiServiceConfig.getBaseUrl();  </span><br><span class="line">        <span class="comment">// 构建完整的请求URL http://localhost:8000/generate?prompt=XXX  </span></span><br><span class="line">        <span class="type">String</span> <span class="variable">url</span> <span class="operator">=</span> String.format(<span class="string">&quot;%s/generate?prompt=%s&quot;</span>, baseUrl, prompt);  </span><br><span class="line">        <span class="comment">// 发送GET请求并获取响应  </span></span><br><span class="line">        <span class="type">GenerateResponse</span> <span class="variable">response</span> <span class="operator">=</span> restTemplate.getForObject(url, GenerateResponse.class);  </span><br><span class="line">        <span class="comment">// 从响应中取出 generated_text 字段值返回  </span></span><br><span class="line">        <span class="keyword">return</span> response != <span class="literal">null</span> ? response.getGenerated_text() : <span class="string">&quot;&quot;</span>;  </span><br><span class="line">    &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="3-本机启动-Demo-前后端工程，测试对话效果"><a href="#3-本机启动-Demo-前后端工程，测试对话效果" class="headerlink" title="3. 本机启动 Demo 前后端工程，测试对话效果"></a>3. 本机启动 Demo 前后端工程，测试对话效果</h5><h6 id="3-1-启动前端工程"><a href="#3-1-启动前端工程" class="headerlink" title="3.1 启动前端工程"></a>3.1 启动前端工程</h6><ul><li>前端项目地址：</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://github.com/huangyf2013320506/magic_conch_frontend.git</span><br></pre></td></tr></table></figure><ul><li>执行：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm run dev</span><br></pre></td></tr></table></figure><h6 id="3-2-启动后端工程"><a href="#3-2-启动后端工程" class="headerlink" title="3.2 启动后端工程"></a>3.2 启动后端工程</h6><ul><li>后端项目地址：</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://github.com/huangyf2013320506/magic_conch_backend.git</span><br></pre></td></tr></table></figure><ul><li>执行：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mvn clean install</span><br></pre></td></tr></table></figure><ul><li>在 <code>MagicConchBackendApplication.java</code> 类中启动 </li></ul><h5 id="4-FastAPI-支持自定义多种请求响应格式，可自行探索"><a href="#4-FastAPI-支持自定义多种请求响应格式，可自行探索" class="headerlink" title="4. FastAPI 支持自定义多种请求响应格式，可自行探索"></a>4. FastAPI 支持自定义多种请求响应格式，可自行探索</h5><h5 id="5-如何开放服务端口到公网"><a href="#5-如何开放服务端口到公网" class="headerlink" title="5. 如何开放服务端口到公网"></a>5. 如何开放服务端口到公网</h5><ul><li>AutoDL 当前仅支持个人用户通过端口转发在本地访问服务，如需开放服务端口到公网一般需要企业认证，请参考：<a href="https://www.autodl.com/docs/port/">开放端口</a></li></ul><h5 id="6-企业部署还需考虑高并发、高可用、安全机制等问题"><a href="#6-企业部署还需考虑高并发、高可用、安全机制等问题" class="headerlink" title="6. 企业部署还需考虑高并发、高可用、安全机制等问题"></a>6. 企业部署还需考虑高并发、高可用、安全机制等问题</h5>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;一、简介&quot;&gt;&lt;a href=&quot;#一、简介&quot; class=&quot;headerlink&quot; title=&quot;一、简介&quot;&gt;&lt;/a&gt;一、简介&lt;/h2&gt;&lt;p&gt;本期视频主要分为以下五部分：&lt;/p&gt;
&lt;h5 id=&quot;1-需求和技术&quot;&gt;&lt;a href=&quot;#1-需求和技术&quot; class=&quot;</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>李沐交大讲座——大语言模型及个人成长分享</title>
    <link href="https://xtaiyang.github.io/posts/990424a1.html"/>
    <id>https://xtaiyang.github.io/posts/990424a1.html</id>
    <published>2025-02-06T11:37:10.000Z</published>
    <updated>2025-03-11T08:14:12.507Z</updated>
    
    <content type="html"><![CDATA[<h1 id="李沐交大讲座——大语言模型及个人成长分享"><a href="#李沐交大讲座——大语言模型及个人成长分享" class="headerlink" title="李沐交大讲座——大语言模型及个人成长分享"></a>李沐交大讲座——大语言模型及个人成长分享</h1><p><img src="https://liuqianglong.com/content/images/size/w2000/2024/10/limu-ai.png" alt="李沐交大讲座——大语言模型及个人成长分享"></p><p>李沐在交大分享了关于大模型的实践经验和未来的预测，以及个人工作和成长的一些经验，感觉很有收货，摘录了一些重点，感兴趣可以看原视频。</p><blockquote><p><strong>李沐</strong>：BosonAI联合创始人，前亚马逊首席科学家，曾任AI创业公司Marianas Labs CTO、百度深度学习研究院主任研发架构师。</p></blockquote><h2 id="大语言模型的现在和的未来"><a href="#大语言模型的现在和的未来" class="headerlink" title="大语言模型的现在和的未来"></a>大语言模型的现在和的未来</h2><p><strong>语言模型的最核心的三个要素是算力、数据和算法</strong>：语言模型像炼丹，数据就像炼丹材料，算力就像炼丹炉等设备，算法就像丹方。以前的深度学习就像一个丹就治一个病，现在希望为丹注入灵魂，解决很多问题。</p><p><strong>带宽</strong>：带宽是最难也是最重要的，大模型分布式训练需要通过光纤连接，光纤目前带宽在400G左右，会成为瓶颈，光纤传输延迟也需要考虑。现在的趋势是把GPU放到一起，距离足够近，英伟达的GB200就是这个思路。密集的GPU会带来电力和散热问题，散热需要使用水冷，水冷对基建有更高的要求。</p><p><strong>内存</strong>：内存比算力更关键，大模型需要大量内存处理数据。当前单芯片内存约 192 GB，内存不够，模型就做不大，模型上限将依赖内存突破。</p><p><strong>算力</strong></p><ul><li>算力提升主要依赖摩尔定律，随着工艺进步和浮点数精度降低（Int4、Int8），硬件效率提升。</li><li>随着模型更大时，供电成为关键问题，自己做数据中心时，曾经计算自己造一个电厂的成本比付的电费成本要低。</li><li>模型分为训练和推理阶段，英伟达在训练芯片上仍处于垄断地位，推理阶段别的芯片还可以。</li><li>尽管推理芯片有替代品，英伟达在训练芯片上仍处于垄断地位。模型训练成本会随时间下降，因此大模型并非长期性价比最高的选择，企业需关注模型的长期价值而非规模。</li></ul><p><strong>训练数据：</strong> OpenAI 或别的开源模型，目前基本都是用10T~50T Token 做预训练。可能弄到更多数据，但是清洗之后可能还是回到这个数值，而且更多的数据不一定带来更好的提升。</p><p><strong>模型尺寸</strong>：未来主流模型将在 100B-500B 参数，500B 以上模型可以训练，但是难以提供服务。保持在这个范围，MoE 虽可用于更大模型，但有效激活规模仍在 500B 左右。</p><p><strong>各领域AI模型发展水平</strong></p><ul><li>语言模型：目前已经达到较高水平，约为80-85分。</li><li>音频模型：已经达到可用水平，评分大约在70-80分之间。</li><li>视频生成模型：尤其是生成具有特定功能的视频，还处于起步阶段，整体水平大约为50分。</li></ul><h2 id="AI-应用现在与挑战"><a href="#AI-应用现在与挑战" class="headerlink" title="AI 应用现在与挑战"></a>AI 应用现在与挑战</h2><p>在 AI 领域目前还没有出现 Killer APP（即短视频类似抖音的APP）。</p><p><strong>三类AI 应用：</strong></p><ul><li>文科白领：AI 在个人助理、Call center、文本处理等领域表现良好，能完成 80-90% 的任务。</li><li>工科白领：AI 可以自动化处理编程任务，如检索和修改代码，但无法完全取代程序员，尤其是复杂任务。</li><li>蓝领阶级：自动驾驶进展较大，但其他蓝领任务（如端盘子、运货）因复杂的物理世界交互难以实现，需要更多数据和技术突破，可能还需 5 年左右。</li></ul><p><strong>垂直模型的误区</strong>：垂直模型是一个伪命题，没有真正的垂直模型，一个很垂直领域的模型，它的通用能力也是不能差的。比如说你要在某一个学科里面拿第一，你别的科目也不能差到哪里去。</p><p><strong>模型评估与数据：</strong> 模型评估非常困难，用人评估比较贵，用模型评估会带来偏差。有一个好的评估可以解决 50% 的问题，解决了评估问题，就能够进行优化了，而且也拥有了一些数据。数据决定模型上限，想让模型在某一个方面做得特别好，需要先把相关数据准备好。</p><p><strong>费用：</strong> 自建机房不一定比租 GPU 便宜太大，因为大头被英伟达吃掉了，它的利润是90%。我是从 Amazon 干了 7 年半才出来创业，但我其实不用 Amazon 服务，太贵了。我们都用小公司买来的，他们当年用来挖比特币的。:)</p><h2 id="自我提升的方法"><a href="#自我提升的方法" class="headerlink" title="自我提升的方法"></a>自我提升的方法</h2><p><strong>大公司：</strong> 在大公司，你要解决问题是公司关心的问题。这一点很重要。大家一定要想清楚：我要在公司干什么，公司今年准备干什么，最好两者保持一致。如果干的事情是自己喜欢的，但不是公司追求的，这就会让人很难受。</p><p><strong>从上级的角度反思自己</strong>：每周总结：你完成了哪些任务？哪些目标未达成？分析原因：是否因为懒惰、方向不对，还是其他因素？</p><p><strong>直面问题</strong>：</p><ul><li><strong>懒惰</strong>：寻找解决方案，比如找学习伙伴进行相互监督，固定时间一起学习或工作。</li><li>不擅长<ul><li><strong>换方向</strong>：如果遇到自己不擅长的领域，考虑调整方向，选择更适合自己的领域。</li><li><strong>加倍努力</strong>：如果无法避免短板，那就花两倍的时间和精力去攻克问题。</li></ul></li></ul><p><strong>对自己狠一点</strong>：自我提升最终比拼的是对自己有多狠，愿意承受多大的努力和坚持。</p><p><strong>养成总结与规划的习惯</strong>：</p><ul><li><strong>每周总结</strong>：每周花30分钟回顾工作，分析不足，规划改进。</li><li><strong>季度回顾</strong>：每季度反思是否实现了既定目标，并根据情况调整下一阶段的计划。</li></ul><p><strong>选择比努力更重要</strong>：做任何选择前，先明确自己的目标。只有目标清晰，才能做出正确的选择。</p><p><strong>定期反思动机</strong>：每年或每五年，反思自己的动机和工作状态。如果过去的一年感觉不开心或没有进展，可能是动机不够强。若是因为时机不成熟，那就继续努力等待；如果是动机不对，及时调整方向。</p><p><strong>讲座B站视频：</strong> <a href="https://www.bilibili.com/video/BV1dHWkewEWz/?vd_source=90d4eedee2acfff3367a55da193a86a0&amp;ref=liuqianglong.com">https://www.bilibili.com/video/BV1dHWkewEWz/?vd_source=90d4eedee2acfff3367a55da193a86a0</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;李沐交大讲座——大语言模型及个人成长分享&quot;&gt;&lt;a href=&quot;#李沐交大讲座——大语言模型及个人成长分享&quot; class=&quot;headerlink&quot; title=&quot;李沐交大讲座——大语言模型及个人成长分享&quot;&gt;&lt;/a&gt;李沐交大讲座——大语言模型及个人成长分享&lt;/h1&gt;&lt;</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Dify创建Flux_AI免费绘图应用</title>
    <link href="https://xtaiyang.github.io/posts/5da1f935.html"/>
    <id>https://xtaiyang.github.io/posts/5da1f935.html</id>
    <published>2025-02-06T11:36:24.000Z</published>
    <updated>2025-03-11T08:14:12.507Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Dify-创建-Flux-AI-免费绘图应用"><a href="#Dify-创建-Flux-AI-免费绘图应用" class="headerlink" title="Dify 创建 Flux AI 免费绘图应用"></a>Dify 创建 Flux AI 免费绘图应用</h1><p><img src="https://liuqianglong.com/content/images/size/w2000/2024/10/Kung_Fu_Panda_holds_a__Dify_with_Flux__banner-_Pix.png" alt="Dify 创建 Flux AI 免费绘图应用"></p><p>本文介绍了如何利用 Dify 创建 Flux AI 免费绘图应用。只需要在 Dify 中输入提示词和图片分辨率，Dify 会直接返回图片。文中还介绍了如何获取硅基流动的免费 API，以及如何自定义 Dify 的绘图插件。</p><h2 id="一、Flux-AI-简介"><a href="#一、Flux-AI-简介" class="headerlink" title="一、Flux AI 简介"></a>一、Flux AI 简介</h2><p>AI 绘画，开源中知名度最高的肯定是 Stable Diffusion。在 2024年8月1日，来自 Stable Diffusion 团队的成员成立了黑森林实验室公司（Black Forest Labs），致力于开发最先进的开源生成模型，用于图像和视频。目前公司有 4 款 AI 绘图模型：[1]</p><ul><li><strong>FLUX1.1 [pro]</strong> ：2024年10月1日发布的最先进且高效的版本，代号“蓝莓”，是目前市面最强的 AI 画图模型（是的，强于Midjourney）。比FLUX.1 [pro]快六倍，同时提升图像质量、提示遵循能力和多样性。</li><li><strong>FLUX.1 [pro]</strong> ：顶级性能图像生成模型，闭源模型，具有最先进的提示遵循能力、视觉质量、细节表现和输出多样性。适用于商业和企业级应用。</li><li><strong>FLUX.1 [dev]</strong> ：是 FLUX.1 [pro] 的蒸馏版本，开源但不可商用，适合非商业应用，至少需要24G显存运行。</li><li><strong>FLUX.1 [schnell]</strong> ：最快速的本地开发和个人使用模型，有120亿参数，完全开源（Apache2.0许可）。和FLUX.1 [dev]一样，权重可在 Hugging Face上获取，并支持 ComfyUI 的集成。</li></ul><p><img src="https://liuqianglong.com/content/images/mypost/ai-flux-drawing-app/image-20241008202232904.png" alt="img"></p><h2 id="二、获取-Flux-免费-API"><a href="#二、获取-Flux-免费-API" class="headerlink" title="二、获取 Flux 免费 API"></a>二、获取 Flux 免费 API</h2><p>目前市面上有很多厂商都提供 Flux 的 API 接口，针对 <code>FLUX.1 [schnell]</code> 模型，因为模型小，有一些厂商提供了免费的 API 额度。下面两个厂商都只需要简单注册即可使用，不需要绑定任何信用卡信息，所以作为推荐。</p><ul><li>硅基流动提供了每分钟调用 2 次，每天 400 次的免费额度（IPM=2 IPD=400）。[2]</li><li>Together.ai 提供了每分钟调用 10 次的免费额度（10 img/min）。[3]</li></ul><p>后面我会利用硅基流动进行演示，其他厂商的对接步骤大同小异。</p><h3 id="Siliconflow"><a href="#Siliconflow" class="headerlink" title="Siliconflow"></a>Siliconflow</h3><p>硅基流动（Siliconflow）是一家北京公司，SiliconCloud 平台提供模型云服务。平台上提供了多种开源大语言模型和图像生成模型，包括 Qwen2.5、DeepSeek-V2.5、Llama-3.1等。用户可以免费使用部分参数较小的模型，例如 Qwen2.5（7B）、Llama3.1（8B）、FLUX.1-schnell 等多个模型的 API 可以免费使用（有限速）。</p><p>这里不演示账号注册过程，注册之后，可以通过官网提供的界面直接体验各个模型。例如，下面使用 <code>FLUX.1 [schnell]</code> 模型进行绘图。[4]</p><p>模型 <code>FLUX.1 [schnell]</code> ，提示词“Kung Fu Panda holds a “Dify with Flux” banner, Pixar style.”，图片比例16:9。<img src="https://liuqianglong.com/content/images/mypost/ai-flux-drawing-app/image-20241008213310901.png" alt="img"></p><p>点击页面左侧“API秘钥”，创建一个秘钥，后续 Dify 自定义工具需要调用这个秘钥信息。<img src="https://liuqianglong.com/content/images/mypost/ai-flux-drawing-app/image-20241008215429313.png" alt="img"></p><h3 id="Together-ai"><a href="#Together-ai" class="headerlink" title="Together.ai"></a>Together.ai</h3><p>Together.ai 是一家成立于2022年6月，总部位于美国硅谷的开源生成式人工智能平台。目前的内容主要是开发去中心化的AI技术，提供开源的AI模型。为各种大模型提供了统一的API接入，包括文本生成和代码生成模型。其中<code>FLUX.1 [schnell]</code> 模型可以免费使用。</p><p>这里不演示账号的注册过程，注册之后，厂商会赠送5美元的额度，可以体验一些付费模型，例如这里使用<code>FLUX.1.1 [pro]</code>模型绘图测试，这个模型的价格是每张图 0.04 美元。[5]</p><p>模型 <code>FLUX.1.1 [pro]</code> ，提示词“Kung Fu Panda holds a “Dify with Flux” banner, Pixar style.”，像素“1024x576”。<img src="https://liuqianglong.com/content/images/mypost/ai-flux-drawing-app/image-20241008211323926.png" alt="img"></p><h2 id="三、Dify-自定义工具"><a href="#三、Dify-自定义工具" class="headerlink" title="三、Dify 自定义工具"></a>三、Dify 自定义工具</h2><p>Dify 没有 FLUX 绘图的插件，这里演示一下如何自定义 FLUX 的绘图插件。</p><p>查看模型 <code>FLUX.1 [schnell]</code>的API文档，拷贝这个请求格式，复制给ChatGPT，让写一个对应的 OpenAPI Schema。[6]<img src="https://liuqianglong.com/content/images/mypost/ai-flux-drawing-app/image-20241009091154057.png" alt="img"></p><p>写给 ChatGPT 的 Prompt。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">curl --request POST \</span><br><span class="line">     --url https://api.siliconflow.cn/v1/black-forest-labs/FLUX.1-schnell/text-to-image \</span><br><span class="line">     --header &#x27;accept: application/json&#x27; \</span><br><span class="line">     --header &#x27;content-type: application/json&#x27; \</span><br><span class="line">     --data &#x27;</span><br><span class="line">&#123;</span><br><span class="line">  &quot;prompt&quot;: &quot;an island near sea, with seagulls, moon shining over the sea, light house, boats int he background, fish flying over the sea&quot;,</span><br><span class="line">  &quot;image_size&quot;: &quot;768x1024&quot;,</span><br><span class="line">  &quot;num_inference_steps&quot;: 20</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">基于上面的内容，写一个OpenAPI Schema</span><br></pre></td></tr></table></figure><p>ChatGPT 的回复，后续复制到 Dify 的自定义工具中去。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line">openapi: 3.0.3</span><br><span class="line">info:</span><br><span class="line">  title: FLUX.1 Schnell Text-to-Image API</span><br><span class="line">  description: This API generates images based on a text prompt.</span><br><span class="line">  version: 1.0.0</span><br><span class="line">servers:</span><br><span class="line">  - url: https://api.siliconflow.cn/v1/black-forest-labs</span><br><span class="line">    description: Production Server</span><br><span class="line">paths:</span><br><span class="line">  /FLUX.1-schnell/text-to-image:</span><br><span class="line">    post:</span><br><span class="line">      summary: Generate an image from a text prompt</span><br><span class="line">      operationId: generateImage</span><br><span class="line">      requestBody:</span><br><span class="line">        required: true</span><br><span class="line">        content:</span><br><span class="line">          application/json:</span><br><span class="line">            schema:</span><br><span class="line">              type: object</span><br><span class="line">              properties:</span><br><span class="line">                prompt:</span><br><span class="line">                  type: string</span><br><span class="line">                  description: The text prompt to generate the image.</span><br><span class="line">                  example: an island near sea, with seagulls, moon shining over the sea, light house, boats in the background, fish flying over the sea</span><br><span class="line">                image_size:</span><br><span class="line">                  type: string</span><br><span class="line">                  description: The desired size of the generated image.</span><br><span class="line">                  example: 768x1024</span><br><span class="line">                num_inference_steps:</span><br><span class="line">                  type: integer</span><br><span class="line">                  description: The number of inference steps for generating the image.</span><br><span class="line">                  example: 20</span><br><span class="line">      responses:</span><br><span class="line">        &#x27;200&#x27;:</span><br><span class="line">          description: Image generated successfully</span><br><span class="line">          content:</span><br><span class="line">            application/json:</span><br><span class="line">              schema:</span><br><span class="line">                type: object</span><br><span class="line">                properties:</span><br><span class="line">                  image_url:</span><br><span class="line">                    type: string</span><br><span class="line">                    description: URL of the generated image</span><br><span class="line">                    example: https://api.siliconflow.cn/v1/black-forest-labs/images/generated-image-12345.png</span><br><span class="line">        &#x27;400&#x27;:</span><br><span class="line">          description: Bad request</span><br><span class="line">          content:</span><br><span class="line">            application/json:</span><br><span class="line">              schema:</span><br><span class="line">                type: object</span><br><span class="line">                properties:</span><br><span class="line">                  error:</span><br><span class="line">                    type: string</span><br><span class="line">                    description: Error message</span><br><span class="line">                    example: Invalid prompt or parameters</span><br><span class="line">      headers:</span><br><span class="line">        accept:</span><br><span class="line">          description: Accepted response format</span><br><span class="line">          required: true</span><br><span class="line">          schema:</span><br><span class="line">            type: string</span><br><span class="line">            example: application/json</span><br><span class="line">        content-type:</span><br><span class="line">          description: Content type of the request</span><br><span class="line">          required: true</span><br><span class="line">          schema:</span><br><span class="line">            type: string</span><br><span class="line">            example: application/json</span><br></pre></td></tr></table></figure><p>在 Dify 平台上，点击“创建自定义工具”，设置工具的名称，复制 ChatGPT 回复的 OpenAPI Schema，点击“测试”。<img src="https://liuqianglong.com/content/images/mypost/ai-flux-drawing-app/image-20241009091620726.png" alt="img"></p><p>设置<code>API Key</code>方式授权，前缀设置为<code>Bearer</code>，输入硅基流动创建的API Key值，点击“保存”（此 API 我已经删除）。<img src="https://liuqianglong.com/content/images/mypost/ai-flux-drawing-app/image-20241009090457294.png" alt="img"></p><p>输入提示词，分辨率和推理步数进行测试，查看返回的测试结果。测试成功，返回了图片的URL。<img src="https://liuqianglong.com/content/images/mypost/ai-flux-drawing-app/image-20241009090502874.png" alt="img"></p><p>拷贝这个测试结果，后续会写一个Python代码，解析这个结果中的 URL 链接，转换成 Markdown 格式，方便 Dify 直接显示图片。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;images&quot;: [&#123;&quot;url&quot;: &quot;https://sf-maas-uat-prod.oss-cn-shanghai.aliyuncs.com/outputs/8c4ecc9a-ccc6-4c01-9f72-874ed33e7125_0.png&quot;&#125;], &quot;timings&quot;: &#123;&quot;inference&quot;: 1.851&#125;, &quot;seed&quot;: 966716722, &quot;shared_id&quot;: &quot;0&quot;&#125;</span><br></pre></td></tr></table></figure><h2 id="四、Dify-创建-AI-绘图工作流"><a href="#四、Dify-创建-AI-绘图工作流" class="headerlink" title="四、Dify 创建 AI 绘图工作流"></a>四、Dify 创建 AI 绘图工作流</h2><p>下面在 Dify 中创建一个工作流，我希望输入提示词和图片分辨率后，就直接接返回图片结果。</p><p>这里一下这个工作流的设计思路。设计有两个变量是必须要提供的，分别是Prompt（提示词）和image_size（图片分辨率）。</p><p>另外还添加了一个可选参数optimize_prompt（提示词优化），这个参数有两个选项On或者Off，如果选择了On，会对提示词进行优化，会丰富提示词的细节。</p><p>如果不选择这个参数，或者选择为Off，会利用GPT4o-mini模型翻译提示词，提示词是英文保持原文，提示词是中文会直接翻译成英文。<strong>Flux 的提示词对中文支持不友好，所以需要翻译为英文</strong>，这里是用GPT4o-mini模型进行翻译，也可以使用其他模型进行翻译。</p><p><img src="https://liuqianglong.com/content/images/mypost/ai-flux-drawing-app/image-20241009100507748.png" alt="img"></p><p>下面是LLM1的提示词，用于优化输入的提示词。</p><p><img src="https://liuqianglong.com/content/images/mypost/ai-flux-drawing-app/image-20241009155857826.png" alt="img"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">You are tasked with expanding prompts for image generation. Your goal is to enhance the input prompt by adding more details, refining context, or specifying elements to make it more vivid and specific. Here are the detailed requirements:</span><br><span class="line"></span><br><span class="line">1. Identify core elements: Determine the key components of the original prompt. These typically include subjects, actions, scene settings, and emotional tones.</span><br><span class="line">2. Enrich with specific details: Add descriptive details to each element, considering the five senses (sight, sound, smell, touch, taste), as well as color, texture, and emotions.</span><br><span class="line">3. Build scenes and imagery: Create scenes by describing the environment, time, or background elements. This helps create a more immersive experience.</span><br><span class="line">4. Use modifiers for enhanced effect: Employ adjectives to vividly describe nouns, and adverbs to precisely modify verbs. This makes the prompt more engaging.</span><br><span class="line">5. Incorporate action and interaction: Where appropriate, describe events taking place in the scene, how characters interact, or the emotional atmosphere permeating the environment.</span><br><span class="line">6. Maintain overall coherence: Ensure the expanded prompt flows naturally and consistently revolves around the original concept.</span><br><span class="line">7. Output the prompt in English: If the content within the &#123;&#123;#1724416537387.Prompt#&#125;&#125; tag is in Chinese, translate it into an English prompt.</span><br><span class="line">Here&#x27;s an example of prompt expansion:</span><br><span class="line">Original prompt: &quot;Forest at sunrise.&quot;</span><br><span class="line">Expanded prompt: &quot;In the heart of an ancient forest, the first light of dawn filters through the dense canopy, casting a golden glow on the dewy moss-covered ground. Tall, towering trees, their bark rough and weathered, stand like silent sentinels as a soft mist curls around their roots. The air is crisp and filled with the earthy scent of pine needles, and the distant call of a waking bird echoes through the tranquil morning.&quot;</span><br><span class="line"></span><br><span class="line">The prompt to be expanded is:</span><br><span class="line">&#123;&#123;#1724416537387.Prompt#&#125;&#125;</span><br></pre></td></tr></table></figure><p>下面是 LLM2 的提示词，用于翻译用户提供的中文提示词。</p><p><img src="https://liuqianglong.com/content/images/mypost/ai-flux-drawing-app/image-20241009155910726.png" alt="img"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;xml&gt;</span><br><span class="line">    &#123;&#123;#1724416537387.Prompt#&#125;&#125;</span><br><span class="line">&lt;/xml&gt;</span><br><span class="line"></span><br><span class="line">If the content within the XML tags is in Chinese, translate it to English. If it is already in English, retain the original text.</span><br></pre></td></tr></table></figure><p>然后进入自定义工具，节点输入的prompt变量来自 LLM 翻译后的提示词，image_size来自最开始选择的图片尺寸，inference_steps 这里设置为固定的值，不用用户填写。这个工具会输出3个变量”text”、“files”和“json”，其中”text”输出的值就是硅基流动返回的响应内容。</p><p><img src="https://liuqianglong.com/content/images/mypost/ai-flux-drawing-app/image-20241009155932447.png" alt="img"></p><p>下面是自定义工具输出的完整字段。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;text&quot;: &quot;&#123;\&quot;images\&quot;: [&#123;\&quot;url\&quot;: \&quot;https://sf-maas-uat-prod.oss-cn-shanghai.aliyuncs.com/outputs/a0616193-e0c4-4fa5-9505-5c8d5f155d2a_0.png\&quot;&#125;], \&quot;timings\&quot;: &#123;\&quot;inference\&quot;: 1.778&#125;, \&quot;seed\&quot;: 868267842, \&quot;shared_id\&quot;: \&quot;0\&quot;&#125;&quot;,</span><br><span class="line">  &quot;files&quot;: [],</span><br><span class="line">  &quot;json&quot;: []</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我希望这个应用直接返回图片，而不是返回图片的 URL 然后再点击链接。Dify支持 Markdown 格式的图片格式，所以需要用一个脚本对输出的字符串进行转换，解析出对应的图片 URL，转换为 Markdown 格式。这里代码添加一个输出变量是键为”arg1”，值为自定义工具输出的”text”字符串。下面是代码执行节点的输入字符串：</p><p><img src="https://liuqianglong.com/content/images/mypost/ai-flux-drawing-app/image-20241009160125121.png" alt="img"></p><p>对于代码执行节点而言，下面是输入的信息：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;arg1&quot;: &quot;&#123;\&quot;images\&quot;: [&#123;\&quot;url\&quot;: \&quot;https://sf-maas-uat-prod.oss-cn-shanghai.aliyuncs.com/outputs/a0616193-e0c4-4fa5-9505-5c8d5f155d2a_0.png\&quot;&#125;], \&quot;timings\&quot;: &#123;\&quot;inference\&quot;: 1.778&#125;, \&quot;seed\&quot;: 868267842, \&quot;shared_id\&quot;: \&quot;0\&quot;&#125;&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里python代码，用与从上面的信息中解析出图片的 URL，并返回 Markdown 格式的图片地址。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>(<span class="params">arg1</span>):</span><br><span class="line">    <span class="comment"># 解析输入的 JSON 字符串</span></span><br><span class="line">    data = json.loads(arg1)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 从解析后的数据中提取图片 URL</span></span><br><span class="line">    image_url = data[<span class="string">&#x27;images&#x27;</span>][<span class="number">0</span>][<span class="string">&#x27;url&#x27;</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 构建 Markdown 格式的图片链接</span></span><br><span class="line">    markdown_image = <span class="string">f&quot;![Image](<span class="subst">&#123;image_url&#125;</span>)&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> &#123;</span><br><span class="line">        <span class="string">&quot;result&quot;</span>: markdown_image</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>按照上面的思路，可以构建其他的 AI 绘图工作流，基本步骤一致。只需要查看其他厂商的请求 API，利用 ChatGPT 转换为 OpenAPI Schema，然后将输出信息解析为 Markdown 格式图片链接即可。</p><h2 id="五、Flux-AI-绘图应用测试"><a href="#五、Flux-AI-绘图应用测试" class="headerlink" title="五、Flux AI 绘图应用测试"></a>五、Flux AI 绘图应用测试</h2><p>下面测试一下这个 AI 绘图应用，输入提示词“Kung Fu Panda holds a “Dify with Flux” banner, Pixar style.”，图片分辨率“1024x576”，关闭提示词优化或者保持为空，点击运行，Dify 可以直接显示生成的图片。</p><p><img src="https://liuqianglong.com/content/images/mypost/ai-flux-drawing-app/image-20241009160356255.png" alt="img"></p><p>点击详情可以查看输出的 Markdown 格式的图片链接信息。</p><p><img src="https://liuqianglong.com/content/images/mypost/ai-flux-drawing-app/image-20241009160406006.png" alt="img"></p><h2 id="六、文档链接"><a href="#六、文档链接" class="headerlink" title="六、文档链接"></a>六、文档链接</h2><ul><li><strong>[1] Announcing Black Forest Labs:</strong> <a href="https://blackforestlabs.ai/announcing-black-forest-labs/?ref=liuqianglong.com">https://blackforestlabs.ai/announcing-black-forest-labs/</a></li><li><strong>[2] 硅基流动速率限制:</strong> <a href="https://docs.siliconflow.cn/rate-limits/rate-limit-and-upgradation?ref=liuqianglong.com#2-1">https://docs.siliconflow.cn/rate-limits/rate-limit-and-upgradation#2-1</a></li><li><strong>[3] Together.ai 速率限制:</strong> <a href="https://docs.together.ai/docs/rate-limits?ref=liuqianglong.com">https://docs.together.ai/docs/rate-limits</a></li><li><strong>[4] 硅基流动文生图体验:</strong> <a href="https://cloud.siliconflow.cn/playground/text-to-image?ref=liuqianglong.com">https://cloud.siliconflow.cn/playground/text-to-image</a></li><li><strong>[5] Together.ai 文生图体验:</strong> <a href="https://api.together.ai/playground/image/black-forest-labs/FLUX.1.1-pro?ref=liuqianglong.com">https://api.together.ai/playground/image/black-forest-labs/FLUX.1.1-pro</a></li><li><strong>[6] 硅基流动 FLUX.1-schnell 模型 API:</strong> <a href="https://docs.siliconflow.cn/reference/imagegeneration?ref=liuqianglong.com">https://docs.siliconflow.cn/reference/imagegeneration</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Dify-创建-Flux-AI-免费绘图应用&quot;&gt;&lt;a href=&quot;#Dify-创建-Flux-AI-免费绘图应用&quot; class=&quot;headerlink&quot; title=&quot;Dify 创建 Flux AI 免费绘图应用&quot;&gt;&lt;/a&gt;Dify 创建 Flux AI 免费绘图</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>HuggingFace模型下载及使用</title>
    <link href="https://xtaiyang.github.io/posts/9c7ceea5.html"/>
    <id>https://xtaiyang.github.io/posts/9c7ceea5.html</id>
    <published>2025-02-06T11:30:30.000Z</published>
    <updated>2025-03-11T08:14:12.507Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Hugging-Face-模型下载及使用"><a href="#Hugging-Face-模型下载及使用" class="headerlink" title="Hugging Face 模型下载及使用"></a>Hugging Face 模型下载及使用</h1><p><img src="https://liuqianglong.com/content/images/size/w2000/2024/08/ai-hugging-face-models.png" alt="Hugging Face 模型下载及使用"></p><p>这篇文章介绍了Hugging Face平台和它的核心产品。演示了如何在AWS EC2实例上，从Hugging Face Hub下载并运行Qwen2-0.5B-Instruct模型。最后，还展示了如何用Gradio图形化界面与Qwen LLM进行聊天对话。</p><h2 id="一、Hugging-Face-简介"><a href="#一、Hugging-Face-简介" class="headerlink" title="一、Hugging Face 简介"></a>一、Hugging Face 简介</h2><p>Hugging Face是一家美国公司，成立于2016年，起初是为青少年开发聊天机器人应用程序。后来，Hugging Face转型为专注于机器学习的平台公司，推出了多款促进NLP（自然语言处理）技术发展的产品。主要产品有：</p><ol><li><strong>预训练模型</strong>：Hugging Face提供了一系列优秀的预训练NLP模型，如BERT、GPT、RoBERTa等，这些模型在多项任务中表现出色。</li><li><strong>Transformers库</strong>：Hugging Face开发了名为<code>transformers</code>的Python库，支持PyTorch和TensorFlow等深度学习框架，提供了加载、微调和使用预训练模型的便捷工具。</li><li><strong>NLP工具</strong>：他们提供了多种NLP相关工具，如文本生成、文本分类和命名实体识别，帮助开发者快速构建NLP应用。</li><li><strong>Hugging Face Hub</strong>：这是一个集中式的Web平台，类似于GitHub，托管基于Git的代码仓库、模型和数据集，并支持项目讨论和拉取请求。</li><li><strong>Hugging Face Spaces</strong>：Hugging Face Spaces是一个允许用户轻松部署和分享AI应用的平台。。它提供了一个易于使用的GUI，使用户能够快速创建和部署Web托管的ML应用。2021年底，Hugging Face宣布收购了Gradio。Gradio是一个开源Python包，允许用户快速为机器学习模型、API或任何Python函数构建交互式演示或Web应用程序，无需编写HTML、CSS或JavaScript代码。</li></ol><h2 id="二、大模型竞技场与排名"><a href="#二、大模型竞技场与排名" class="headerlink" title="二、大模型竞技场与排名"></a>二、大模型竞技场与排名</h2><p>另外在Hugging Face可以查看各个大模型的排行榜，例如下面的开源大模型排行榜 [1]。</p><p><img src="https://liuqianglong.com/content/images/mypost/ai-hugging-face-models/image-20240827104909122.png" alt="img"></p><p>由LMSYS维护的大模型聊天竞技场，收集人类对大模型聊天回复的反馈，进行排名 [2]。</p><p><img src="https://liuqianglong.com/content/images/mypost/ai-hugging-face-models/image-20240827104935909.png" alt="img"></p><p>发送的问题会同时给两个模型进行回复，根据回复内容进行投票，一共4个选项，A胜、B胜、平手或者都不行。投票后会显示回复的大模型版本。</p><p><img src="https://liuqianglong.com/content/images/mypost/ai-hugging-face-models/image-20240705153257611.png" alt="img"></p><h2 id="三、Hugging-Face-Spaces-使用"><a href="#三、Hugging-Face-Spaces-使用" class="headerlink" title="三、Hugging Face Spaces 使用"></a>三、Hugging Face Spaces 使用</h2><p>Hugging Face Spaces是一个允许用户轻松部署和分享AI应用的平台，很多大模型都会在Spaces上发布不同版本的模型，为大家提供测试。例如下面是Qwen的Spaces空间<a href="https://huggingface.co/Qwen?ref=liuqianglong.com">https://huggingface.co/Qwen</a> [3]。</p><p><img src="https://liuqianglong.com/content/images/mypost/ai-hugging-face-models/image-20240705113112506.png" alt="img"></p><p>点击进入<code>Qwen2-72B-Instruct Chat</code>，可以进行聊天测试。</p><p><img src="https://liuqianglong.com/content/images/mypost/ai-hugging-face-models/image-20240705135420942.png" alt="img"></p><h2 id="四、EC2-本地运行-Qwen2-0-5B-Instruct-模型"><a href="#四、EC2-本地运行-Qwen2-0-5B-Instruct-模型" class="headerlink" title="四、EC2 本地运行 Qwen2-0.5B-Instruct 模型"></a>四、EC2 本地运行 Qwen2-0.5B-Instruct 模型</h2><p>这部分演示一下，通过EC2下载和启动<code>Qwen2-0.5B-Instruct</code>模型。我选择Deep Learning类型的AMI，这个类型的AMI已经预先安装了英伟达的驱动。</p><p><img src="https://liuqianglong.com/content/images/mypost/ai-hugging-face-models/image-20240705135658365.png" alt="img"></p><p>选择Ubuntu或者Amazon Linux2023都可以，这里我选择Ubuntu22.04版本的镜像。</p><p><img src="https://liuqianglong.com/content/images/mypost/ai-hugging-face-models/image-20240705135706563.png" alt="img"></p><p>选择<code>g4dn.xlarge</code>，带有1个NVIDA T4显卡，共有16GB GPU，这是带有英伟达显卡最便宜的实例。</p><p><img src="https://liuqianglong.com/content/images/mypost/ai-hugging-face-models/image-20240705135737159.png" alt="img"></p><p>实例已经预装了英伟达驱动，启动之后通过<code>nvidia-smi</code>命令查看一下GPU信息。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">root@ip-172-31-83-158:~# nvidia-smi</span><br><span class="line">Fri Jul  5 03:34:19 2024</span><br><span class="line">+---------------------------------------------------------------------------------------+</span><br><span class="line">| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |</span><br><span class="line">|-----------------------------------------+----------------------+----------------------+</span><br><span class="line">| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |</span><br><span class="line">| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |</span><br><span class="line">|                                         |                      |               MIG M. |</span><br><span class="line">|=========================================+======================+======================|</span><br><span class="line">|   0  Tesla T4                       On  | 00000000:00:1E.0 Off |                    0 |</span><br><span class="line">| N/A   33C    P8               9W /  70W |      2MiB / 15360MiB |      0%      Default |</span><br><span class="line">|                                         |                      |                  N/A |</span><br><span class="line">+-----------------------------------------+----------------------+----------------------+</span><br><span class="line">+---------------------------------------------------------------------------------------+</span><br><span class="line">| Processes:                                                                            |</span><br><span class="line">|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |</span><br><span class="line">|        ID   ID                                                             Usage      |</span><br><span class="line">|=======================================================================================|</span><br><span class="line">|  No running processes found                                                           |</span><br><span class="line">+---------------------------------------------------------------------------------------+</span><br></pre></td></tr></table></figure><p>进入Qwen2-0.5B-Instruct页面<a href="https://huggingface.co/Qwen/Qwen2-0.5B-Instruct?ref=liuqianglong.com">https://huggingface.co/Qwen/Qwen2-0.5B-Instruct</a> ，拷贝模型链接 [5]。<img src="https://liuqianglong.com/content/images/mypost/ai-hugging-face-models/image-20240705144629858.png" alt="img"></p><p>要下载模型文件，需要先安装<code>git-lfs</code>，否则不会下载项目中的大型文件。</p><blockquote><p>Git LFS（Large File Storage）是一个Git扩展工具，用于高效管理大文件。它通过将大文件存储在单独的LFS存储库中，避免了Git仓库体积过大，提升了项目管理效率。Git LFS特别适用于需要处理大型文件的项目，如音乐、图片、视频等。该工具易于安装和配置，并且在多个平台上有良好的支持。</p></blockquote><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">root@ip-172-31-83-158:~# apt update -y</span><br><span class="line">root@ip-172-31-83-158:~# apt install git-lfs -y</span><br></pre></td></tr></table></figure><p>下载模型文件</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">root@ip-172-31-83-158:~# git clone https://huggingface.co/Qwen/Qwen2-0.5B-Instruct</span><br></pre></td></tr></table></figure><p>查看模型文件</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">root@ip-172-31-83-158:~# cd Qwen2-0.5B-Instruct/</span><br><span class="line"></span><br><span class="line">root@ip-172-31-83-158:~/Qwen2-0.5B-Instruct# ll -h</span><br><span class="line">total 954M</span><br><span class="line">drwxr-xr-x 3 root root 4.0K Jul  5 03:36 ./</span><br><span class="line">drwx------ 8 root root 4.0K Jul  5 03:36 ../</span><br><span class="line">drwxr-xr-x 9 root root 4.0K Jul  5 03:36 .git/</span><br><span class="line">-rw-r--r-- 1 root root 1.5K Jul  5 03:36 .gitattributes</span><br><span class="line">-rw-r--r-- 1 root root  12K Jul  5 03:36 LICENSE</span><br><span class="line">-rw-r--r-- 1 root root 3.5K Jul  5 03:36 README.md</span><br><span class="line">-rw-r--r-- 1 root root  659 Jul  5 03:36 config.json</span><br><span class="line">-rw-r--r-- 1 root root  242 Jul  5 03:36 generation_config.json</span><br><span class="line">-rw-r--r-- 1 root root 1.6M Jul  5 03:36 merges.txt</span><br><span class="line">-rw-r--r-- 1 root root 943M Jul  5 03:36 model.safetensors</span><br><span class="line">-rw-r--r-- 1 root root 6.8M Jul  5 03:36 tokenizer.json</span><br><span class="line">-rw-r--r-- 1 root root 1.3K Jul  5 03:36 tokenizer_config.json</span><br><span class="line">-rw-r--r-- 1 root root 2.7M Jul  5 03:36 vocab.json</span><br></pre></td></tr></table></figure><p>安装python包依赖，这三个python包都比较大，下载时间较长。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">root@ip-172-31-83-158:~# pip install transformers torch accelerate</span><br></pre></td></tr></table></figure><p>国内可以指定pip安装源。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install transformers torch -i https://pypi.tuna.tsinghua.edu.cn/simple</span><br></pre></td></tr></table></figure><p>拷贝官方提供的快速启动脚本，这里我修改模型的路径为绝对路径<code>&quot;/root/Qwen2-0.5B-Instruct&quot;</code>，Prompt修改为<code>&quot;你是谁？&quot;</code>，增加了打印响应结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">vim /root/qwen0<span class="number">.5</span>b-demo.py</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForCausalLM, AutoTokenizer</span><br><span class="line">device = <span class="string">&quot;cuda&quot;</span> <span class="comment"># the device to load the model onto</span></span><br><span class="line"></span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(</span><br><span class="line">    <span class="string">&quot;/root/Qwen2-0.5B-Instruct&quot;</span>,</span><br><span class="line">    torch_dtype=<span class="string">&quot;auto&quot;</span>,</span><br><span class="line">    device_map=<span class="string">&quot;auto&quot;</span></span><br><span class="line">)</span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;Qwen/Qwen2-0.5B-Instruct&quot;</span>)</span><br><span class="line"></span><br><span class="line">prompt = <span class="string">&quot;你是谁？&quot;</span></span><br><span class="line">messages = [</span><br><span class="line">    &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;You are a helpful assistant.&quot;</span>&#125;,</span><br><span class="line">    &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: prompt&#125;</span><br><span class="line">]</span><br><span class="line">text = tokenizer.apply_chat_template(</span><br><span class="line">    messages,</span><br><span class="line">    tokenize=<span class="literal">False</span>,</span><br><span class="line">    add_generation_prompt=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line">model_inputs = tokenizer([text], return_tensors=<span class="string">&quot;pt&quot;</span>).to(device)</span><br><span class="line"></span><br><span class="line">generated_ids = model.generate(</span><br><span class="line">    model_inputs.input_ids,</span><br><span class="line">    max_new_tokens=<span class="number">512</span></span><br><span class="line">)</span><br><span class="line">generated_ids = [</span><br><span class="line">    output_ids[<span class="built_in">len</span>(input_ids):] <span class="keyword">for</span> input_ids, output_ids <span class="keyword">in</span> <span class="built_in">zip</span>(model_inputs.input_ids, generated_ids)</span><br><span class="line">]</span><br><span class="line">response = tokenizer.batch_decode(generated_ids, skip_special_tokens=<span class="literal">True</span>)[<span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span>(response)</span><br></pre></td></tr></table></figure><p>运行脚本，查看模型返回结果。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">root@ip-172-31-83-158:~# python3 qwen0.5b-demo.py</span><br><span class="line">Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.</span><br><span class="line">The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you s `attention_mask` to obtain reliable results.</span><br><span class="line">我是阿里云开发的一款超大规模语言模型，我叫通义千问。</span><br></pre></td></tr></table></figure><p>查看GPU消耗。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">ubuntu@ip-172-31-83-158:~$ nvidia-smi</span><br><span class="line">Fri Jul  5 03:51:05 2024</span><br><span class="line">+---------------------------------------------------------------------------------------+</span><br><span class="line">| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |</span><br><span class="line">|-----------------------------------------+----------------------+----------------------+</span><br><span class="line">| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |</span><br><span class="line">| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |</span><br><span class="line">|                                         |                      |               MIG M. |</span><br><span class="line">|=========================================+======================+======================|</span><br><span class="line">|   0  Tesla T4                       On  | 00000000:00:1E.0 Off |                    0 |</span><br><span class="line">| N/A   37C    P0              32W /  70W |   1313MiB / 15360MiB |      0%      Default |</span><br><span class="line">|                                         |                      |                  N/A |</span><br><span class="line">+-----------------------------------------+----------------------+----------------------+</span><br><span class="line"></span><br><span class="line">+---------------------------------------------------------------------------------------+</span><br><span class="line">| Processes:                                                                            |</span><br><span class="line">|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |</span><br><span class="line">|        ID   ID                                                             Usage      |</span><br><span class="line">|=======================================================================================|</span><br><span class="line">|    0   N/A  N/A      3152      C   python3                                    1308MiB |</span><br><span class="line">+---------------------------------------------------------------------------------------+</span><br></pre></td></tr></table></figure><p><strong>Qwen Gradio 图形化界面</strong></p><p>命令行进行对话并不方便，可以使用官方提供的<code>web_demo.py</code>脚本 [6]，通过图形化页面进行对话，脚本网址是<a href="https://github.com/QwenLM/Qwen2/blob/main/examples/demo/web_demo.py。?ref=liuqianglong.com">https://github.com/QwenLM/Qwen2/blob/main/examples/demo/web_demo.py。</a></p><p>安装脚本所需要的依赖，前面已经安装过<code>transformers torch accelerate</code>这三个库了，这里只需要安装<code>gradio</code>即可。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">root@ip-172-31-83-158:~# pip install gradio</span><br></pre></td></tr></table></figure><p>运行脚本，<code>-- server-name 0.0.0.0</code>允许所有地址进行访问，<code>--checkpoint-path /root/Qwen2-0.5B-Instruct</code>指定模型文件所在目录。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">root@ip-172-31-83-158:~# python3 web_demo.py --server-name 0.0.0.0 --checkpoint-path /root/Qwen2-0.5B-Instruct</span><br></pre></td></tr></table></figure><p>使用公网地址加上8000端口打开网页查看。</p><p><img src="https://liuqianglong.com/content/images/mypost/ai-hugging-face-models/image-20240705150306891.png" alt="img"></p><h2 id="五、文档链接"><a href="#五、文档链接" class="headerlink" title="五、文档链接"></a>五、文档链接</h2><ul><li>[1] 开源大模型排行榜：<a href="https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard?ref=liuqianglong.com">https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard</a></li><li>[2] 大模型聊天竞技场： <a href="https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard?ref=liuqianglong.com">https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard</a></li><li>[3] Qwen 的 Spaces 空间：<a href="https://huggingface.co/Qwen?ref=liuqianglong.com">https://huggingface.co/Qwen</a></li><li>[4] Qwen2-72B-Instruct Spaces 空间：<a href="https://huggingface.co/spaces/Qwen/Qwen2-72B-Instruct?ref=liuqianglong.com">https://huggingface.co/spaces/Qwen/Qwen2-72B-Instruct</a></li><li>[5] Qwen2-0.5B-Instruct 模型下载地址： <a href="https://huggingface.co/Qwen/Qwen2-0.5B-Instruct?ref=liuqianglong.com">https://huggingface.co/Qwen/Qwen2-0.5B-Instruct</a></li><li>[6] Qwen2 网页demo代码 <a href="https://github.com/QwenLM/Qwen2/blob/main/examples/demo/web_demo.py?ref=liuqianglong.com">https://github.com/QwenLM/Qwen2/blob/main/examples/demo/web_demo.py</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Hugging-Face-模型下载及使用&quot;&gt;&lt;a href=&quot;#Hugging-Face-模型下载及使用&quot; class=&quot;headerlink&quot; title=&quot;Hugging Face 模型下载及使用&quot;&gt;&lt;/a&gt;Hugging Face 模型下载及使用&lt;/h1&gt;&lt;</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Ollama结合Open-WebUI本地运行大模型</title>
    <link href="https://xtaiyang.github.io/posts/8a046b75.html"/>
    <id>https://xtaiyang.github.io/posts/8a046b75.html</id>
    <published>2025-02-06T11:28:39.000Z</published>
    <updated>2025-03-11T08:14:12.507Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Ollama-结合-Open-WebUI-本地运行大模型"><a href="#Ollama-结合-Open-WebUI-本地运行大模型" class="headerlink" title="Ollama 结合 Open-WebUI 本地运行大模型"></a>Ollama 结合 Open-WebUI 本地运行大模型</h1><p><img src="https://liuqianglong.com/content/images/size/w2000/2024/08/ollama-openwebui-llm.png" alt="Ollama 结合 Open-WebUI 本地运行大模型"></p><p>本文介绍了如何使用 Ollama 在本地运行大型语言模型，以及利用 Open-WebUI 提供的图形化界面与大语言模型进行交互。</p><h2 id="一、Ollama-简介"><a href="#一、Ollama-简介" class="headerlink" title="一、Ollama 简介"></a>一、Ollama 简介</h2><p>Ollama 是一个开源框架，专门设计用于在本地运行大型语言模型（LLM）。它的主要特点和功能如下：</p><ul><li><strong>简化部署</strong>：Ollama 旨在简化在 Docker 容器中部署 LLM 的过程，使得管理和运行这些模型变得更加容易。安装完成后，用户可以通过简单的命令行操作启动和运行大型语言模型。例如，要运行 Gemma 2B 模型，只需执行命令 <code>ollama run gemma:2b</code>。</li><li><strong>捆绑模型组件</strong>：它将模型权重、配置和数据捆绑到一个包中，称为 Modelfile，这有助于优化设置和配置细节，包括 GPU 使用情况。</li><li><strong>支持多种模型</strong>：Ollama 支持多种大型语言模型，如 Llama 2、Code Llama、Mistral、Gemma 等，并允许用户根据特定需求定制和创建自己的模型。</li><li><strong>跨平台支持</strong>：支持 Windows、macOS 和 Linux 平台。安装过程简单，用户只需访问 Ollama 的官方网站下载相应平台的安装包即可。</li></ul><h2 id="二、Docker安装-Ollama"><a href="#二、Docker安装-Ollama" class="headerlink" title="二、Docker安装 Ollama"></a>二、Docker安装 Ollama</h2><p>选择Deep Learning类型的AMI，这个类型的AMI已经预先安装了英伟达的驱动。</p><p><img src="https://liuqianglong.com/content/images/mypost/ai-ollama-openwebui-llm/image-20240705135706563.png" alt="img"></p><p>选择<code>g4dn.xlarge</code>类型实例，带有1个NVIDA T4显卡，共有16GB GPU，这是带有英伟达显卡最便宜的实例。这个AMI已经预装了docker包，如果没有安装的Ubuntu系列系统，可以通过下面命令安装。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">apt update -y</span><br><span class="line">apt install docker -y</span><br><span class="line">systemctl start docker</span><br></pre></td></tr></table></figure><p>拉取ollama docker镜像（依赖网速，2-3分钟左右）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker pull ollama/ollama</span><br></pre></td></tr></table></figure><p>查看镜像文件，镜像大概2GB左右。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">~# </span><span class="language-bash">docker images</span></span><br><span class="line">REPOSITORY      TAG       IMAGE ID       CREATED        SIZE</span><br><span class="line">ollama/ollama   latest    d5cbea22fd07   30 hours ago   1.98GB</span><br></pre></td></tr></table></figure><p>根据Docker Hub ollama主页 <a href="https://hub.docker.com/r/ollama/ollama?ref=liuqianglong.com">https://hub.docker.com/r/ollama/ollama</a> [1]，快速启动容器，注意，<strong>这里没有添加GPU参数，是通过<code>CPU</code>加载启动的</strong>。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">~# </span><span class="language-bash">docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama</span></span><br></pre></td></tr></table></figure><p>查看容器状态。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">root@ip-172-31-83-158:~# docker ps</span><br><span class="line">CONTAINER ID   IMAGE           COMMAND               CREATED         STATUS         PORTS                                           NAMES</span><br><span class="line">e43072764685   ollama/ollama   &quot;/bin/ollama serve&quot;   6 seconds ago   Up 5 seconds   0.0.0.0:11434-&gt;11434/tcp, :::11434-&gt;11434/tcp   ollama</span><br></pre></td></tr></table></figure><p>在docker容器内运行模型（1分钟左右拉起来）。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">root@ip-172-31-83-158:~# docker exec -it ollama bash</span><br><span class="line"></span><br><span class="line">root@e43072764685:/# ollama run qwen2:0.5b</span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; 你叫什么？</span></span><br><span class="line">我叫通义千问。</span><br></pre></td></tr></table></figure><p>新开一个SSH窗口，在模型回答问题时，查看通过<code>top</code>命令，查看CPU使用率，可以看到CPU已经满负荷运行了，正在使用CPU进行推理。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">top - 08:41:19 up  5:14,  4 users,  load average: 0.32, 0.25, 0.13</span><br><span class="line">Tasks: 161 total,   3 running, 158 sleeping,   0 stopped,   0 zombie</span><br><span class="line"><span class="meta prompt_">%</span><span class="language-bash">Cpu(s): 31.9 us,  0.2 sy,  0.0 ni, 67.9 <span class="built_in">id</span>,  0.0 wa,  0.0 hi,  0.0 si,  0.1 st</span></span><br><span class="line">MiB Mem :  15779.1 total,   1575.5 free,    518.2 used,  13685.4 buff/cache</span><br><span class="line">MiB Swap:      0.0 total,      0.0 free,      0.0 used.  14806.4 avail Mem</span><br><span class="line"></span><br><span class="line">    PID USER      PR  NI    VIRT    RES    SHR S  %CPU  %MEM     TIME+ COMMAND</span><br><span class="line">   7080 root      20   0 1067764 442088 344752 R 123.3   2.7   0:17.46 ollama_llama_se</span><br><span class="line">   7006 root      20   0 2365904 400032 389632 S   1.0   2.5   0:13.81 ollama</span><br><span class="line">   6986 root      20   0 1238716  13636   9856 S   0.7   0.1   0:00.23 containerd-shim</span><br></pre></td></tr></table></figure><p>通过<code>ollama list</code>命令，查看下载的所有模型。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">root@e43072764685:/# ollama list</span><br><span class="line">NAME            ID              SIZE    MODIFIED</span><br><span class="line">qwen2:0.5b      6f48b936a09f    352 MB  About a minute ago</span><br></pre></td></tr></table></figure><p>在模型回答问题时，通过<code>nvidia-smi</code>命令查看GPU使用情况，通过CPU进行推理时，看到并没有暂用GPU资源。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">root@ip-172-31-83-158:~# nvidia-smi</span><br><span class="line">Fri Jul  5 08:41:38 2024</span><br><span class="line">+---------------------------------------------------------------------------------------+</span><br><span class="line">| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |</span><br><span class="line">|-----------------------------------------+----------------------+----------------------+</span><br><span class="line">| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |</span><br><span class="line">| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |</span><br><span class="line">|                                         |                      |               MIG M. |</span><br><span class="line">|=========================================+======================+======================|</span><br><span class="line">|   0  Tesla T4                       On  | 00000000:00:1E.0 Off |                    0 |</span><br><span class="line">| N/A   33C    P8               9W /  70W |      2MiB / 15360MiB |      0%      Default |</span><br><span class="line">|                                         |                      |                  N/A |</span><br><span class="line">+-----------------------------------------+----------------------+----------------------+</span><br><span class="line"></span><br><span class="line">+---------------------------------------------------------------------------------------+</span><br><span class="line">| Processes:                                                                            |</span><br><span class="line">|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |</span><br><span class="line">|        ID   ID                                                             Usage      |</span><br><span class="line">|=======================================================================================|</span><br><span class="line">|  No running processes found                                                           |</span><br><span class="line">+---------------------------------------------------------------------------------------+</span><br></pre></td></tr></table></figure><p>删除容器。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">root@ip-172-31-83-158:~#  docker rm -f ollama</span><br><span class="line">ollama</span><br></pre></td></tr></table></figure><p>重启启动ollama容器，通过<strong>GPU</strong>启动。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">root@ip-172-31-83-158:~# docker run -d --gpus=all -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama</span><br><span class="line">b094349fc98c8dee31640485ed88ecef38202e33baadcbb9d503ecc1055ac974</span><br></pre></td></tr></table></figure><p><strong>Ollama 环境变量</strong></p><p>通过下面命令，ollama可以加载环境变量，可以根据实际需求决定是否添加对应的环境变量。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -d --gpus=all -v ollama:/root/.ollama -p 11434:11434 --name ollama --restart always -e OLLAMA_KEEP_ALIVE=-1 ollama/ollama</span><br></pre></td></tr></table></figure><blockquote><p>ollama常用环境变量：</p><ul><li>OLLAMA_KEEP_ALIVE=-1，模型加载后，默认的Keepalive是5分钟，修改为-1可以让模型持续启动，OLLAMA_KEEP_ALIVE=60 表示 60 秒，OLLAMA_KEEP_ALIVE=10m 表示10分钟。<a href="https://github.com/ollama/ollama/pull/3094?ref=liuqianglong.com">https://github.com/ollama/ollama/pull/3094</a></li><li>OLLAMA_ORIGINS=*，跨域访问环境变量，可以用与沉浸式翻译。</li></ul></blockquote><p><strong>Ollama 升级</strong></p><p>ollama升级过程参考，停止容器后，重新拉取容器启动即可。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">root@ip-172-31-83-158:~# docker exec -it ollama bash</span><br><span class="line">root@3d1be8deba41:/# ollama -v</span><br><span class="line">ollama version is 0.1.40</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">(base) root@ip-172-31-79-195:~# docker pull ollama/ollama</span><br><span class="line">Using default tag: latest</span><br><span class="line">latest: Pulling from ollama/ollama</span><br><span class="line">7646c8da3324: Pull complete</span><br><span class="line">d1060ab4fb75: Pull complete</span><br><span class="line">e58f7d737fbb: Pull complete</span><br><span class="line">Digest: sha256:4a3c5b5261f325580d7f4f6440e5094d807784f0513439dcabfda9c2bdf4191e</span><br><span class="line">Status: Downloaded newer image for ollama/ollama:latest</span><br><span class="line">docker.io/ollama/ollama:latest</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">(base) root@ip-172-31-79-195:~# docker run -d --gpus=all -v ollama:/root/.ollama -p 11434:11434 -e OLLAMA_ORIGINS=* --name ollama --restart always  ollama/ollama</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">root@46648320fcd4:/# ollama -v</span><br><span class="line">ollama version is 0.3.7</span><br></pre></td></tr></table></figure><h2 id="三、Open-WebUI"><a href="#三、Open-WebUI" class="headerlink" title="三、Open-WebUI"></a>三、Open-WebUI</h2><p>OpenWebUI是一个可扩展、功能丰富且用户友好的自托管WebUI，它支持完全离线操作，并兼容Ollama和OpenAI的API。这为用户提供了一个可视化的界面，使得与大型语言模型的交互更加直观和便捷 [3]。</p><p>从docker hub拉取open-webui镜像（文件1G左右，看网速，2-3分钟）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker pull dyrnq/open-webui:main</span><br></pre></td></tr></table></figure><p>注意，官方文档是从 GitHub Container Registry (GHCR) 上拉取镜像，而不是从 Docker Hub。官方文档拉取命令如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker pull ghcr.io/open-webui/open-webui:main</span><br></pre></td></tr></table></figure><p>查看镜像</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">root@ip-172-31-83-158:~# docker images</span><br><span class="line">REPOSITORY         TAG       IMAGE ID       CREATED         SIZE</span><br><span class="line">dyrnq/open-webui   main      8aa8279a3e25   10 hours ago   3.9GB</span><br><span class="line">ollama/ollama      latest    d5cbea22fd07   30 hours ago   1.98GB</span><br></pre></td></tr></table></figure><p>启动docker镜像，映射出来3000端口。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">root@ip-172-31-83-158:~# docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always dyrnq/open-webui:main</span><br></pre></td></tr></table></figure><p>查看EC2的公网IP地址。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">root@ip-172-31-83-158:~# curl ifconfig.me</span><br><span class="line">54.243.6.37</span><br></pre></td></tr></table></figure><p>访问Open WebUI。</p><p><img src="https://liuqianglong.com/content/images/mypost/ai-ollama-openwebui-llm/image-20240705165219856.png" alt="img"></p><p>创建账号，这个是本地账号，随便添加账号信息即可。</p><p><img src="https://liuqianglong.com/content/images/mypost/ai-ollama-openwebui-llm/image-20240705165223559.png" alt="img"></p><p>选择ollama中的模型，聊天测试。</p><p><img src="https://liuqianglong.com/content/images/mypost/ai-ollama-openwebui-llm/image-20240705165228399.png" alt="img"></p><p>可以直接拉取目前ollama没有的模型。</p><p><img src="https://liuqianglong.com/content/images/mypost/ai-ollama-openwebui-llm/image-20240705165234184.png" alt="img"></p><p>与下载的新模型进行对话。</p><p><img src="https://liuqianglong.com/content/images/mypost/ai-ollama-openwebui-llm/image-20240828110808614.png" alt="img"></p><h2 id="四、文档链接"><a href="#四、文档链接" class="headerlink" title="四、文档链接"></a>四、文档链接</h2><ul><li>[1] Dockerhub ollama镜像文件：<a href="https://hub.docker.com/r/ollama/ollama?ref=liuqianglong.com">https://hub.docker.com/r/ollama/ollama</a></li><li>[2] ollama keepalive：<a href="https://github.com/ollama/ollama/pull/3094?ref=liuqianglong.com">https://github.com/ollama/ollama/pull/3094</a></li><li>[3] Open WebUI Github：<a href="https://github.com/open-webui/open-webui?ref=liuqianglong.com">https://github.com/open-webui/open-webui</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Ollama-结合-Open-WebUI-本地运行大模型&quot;&gt;&lt;a href=&quot;#Ollama-结合-Open-WebUI-本地运行大模型&quot; class=&quot;headerlink&quot; title=&quot;Ollama 结合 Open-WebUI 本地运行大模型&quot;&gt;&lt;/a&gt;Ol</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Xinference本地运行大模型</title>
    <link href="https://xtaiyang.github.io/posts/92cb901f.html"/>
    <id>https://xtaiyang.github.io/posts/92cb901f.html</id>
    <published>2025-02-06T11:27:13.000Z</published>
    <updated>2025-03-11T08:14:12.507Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Xinference-本地运行大模型"><a href="#Xinference-本地运行大模型" class="headerlink" title="Xinference 本地运行大模型"></a>Xinference 本地运行大模型</h1><p><img src="https://liuqianglong.com/content/images/size/w2000/2024/09/ai-xinference-local-model.png" alt="Xinference 本地运行大模型"></p><p>本文介绍了如何使用 Docker 部署 Xinference 推理框架，并演示了如何启动和运行多种大模型，包括大语言模型、图像生成模型和多模态模型。还讲解了嵌入和重排模型的启动方法，为后续 Dify 调用嵌入和重排模型做为铺垫。</p><h2 id="一、Xinference-简介"><a href="#一、Xinference-简介" class="headerlink" title="一、Xinference 简介"></a>一、Xinference 简介</h2><p>Xorbits Inference (Xinference) 是一个开源的分布式推理框架，专为大规模模型推理任务设计。它支持大语言模型（LLM）、多模态模型、语音识别模型等多种模型的推理。以下是 Xinference 的主要特点 [1]：</p><ul><li><strong>模型一键部署</strong>：极大简化了大语言模型、多模态模型和语音识别模型的部署过程。</li><li><strong>内置前沿模型</strong>：支持一键下载并部署大量前沿开源模型，如 <code>Qwen2</code>、<code>chatglm2</code>、等。</li><li><strong>异构硬件支持</strong>：可以利用 CPU 和 GPU 进行推理，提升集群吞吐量和降低延迟。</li><li><strong>灵活的 API</strong>：提供包括 RPC 和 RESTful API 在内的多种接口，兼容 OpenAI 协议，方便与现有系统集成。</li><li><strong>分布式架构</strong>：支持跨设备和跨服务器的分布式部署，允许高并发推理，并简化扩容和缩容操作。</li><li><strong>第三方集成</strong>：与 LangChain 等流行库无缝对接，快速构建基于 AI 的应用程序。</li></ul><h2 id="二、Xinference-Docker-部署"><a href="#二、Xinference-Docker-部署" class="headerlink" title="二、Xinference Docker 部署"></a>二、Xinference Docker 部署</h2><p>docker镜像文件非常大，拉取文件需要耗费很长时间。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker pull xprobe/xinference</span><br></pre></td></tr></table></figure><p>查看xinference docker镜像文件，目前大小为17.7GB。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">root@ip-172-31-83-158:~# docker images</span><br><span class="line">REPOSITORY          TAG       IMAGE ID       CREATED         SIZE</span><br><span class="line">xprobe/xinference   latest    96b2be814b0f   2 days ago     17.6GB</span><br></pre></td></tr></table></figure><p>创建一个目录，用与存放xinference缓存文件和日志文件。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /xinference/data</span><br></pre></td></tr></table></figure><p>启动容器。默认情况下，镜像中不包含任何模型文件，使用过程中会在容器内下载模型。如果需要使用已经下载好的模型，需要将宿主机的目录挂载到容器内。这种情况下，需要在运行容器时指定本地卷，并且为 Xinference 配置环境变量。</p><ul><li><strong>XINFERENCE_MODEL_SRC</strong>：配置模型下载仓库。默认下载源是 “huggingface”，也可以设置为 “modelscope” 作为下载源。</li><li><strong>XINFERENCE_HOME</strong>：Xinference 默认使用 <code>&lt;HOME&gt;/.xinference</code> 作为默认目录来存储模型以及日志等必要的文件。其中 <code>&lt;HOME&gt;</code> 是当前用户的主目录。可以通过配置这个环境变量来修改默认目录。</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">docker run -d \</span><br><span class="line">  --name xinference \</span><br><span class="line">  -v /xinference/data/.xinference:/root/.xinference \</span><br><span class="line">  -v /xinference/data/.cache/huggingface:/root/.cache/huggingface \</span><br><span class="line">  -v /xinference/data/.cache/modelscope:/root/.cache/modelscope \</span><br><span class="line">  -v /xinference/log:/workspace/xinference/logs \</span><br><span class="line">  -e XINFERENCE_HOME=/xinference \</span><br><span class="line">  -p 9997:9997 \</span><br><span class="line">  --gpus all \</span><br><span class="line">  xprobe/xinference:latest \</span><br><span class="line">  xinference-local -H 0.0.0.0 --log-level debug</span><br></pre></td></tr></table></figure><h2 id="三、Xinference-本地运行大模型"><a href="#三、Xinference-本地运行大模型" class="headerlink" title="三、Xinference 本地运行大模型"></a>三、Xinference 本地运行大模型</h2><p>容器启动后，访问公网地址加上9997端口，启动qwen2-instruct模型。</p><p><img src="https://liuqianglong.com/content/images/mypost/ai-xinference-local-model/image-20240705172918283.png" alt="image-20240705172918283"></p><p>使用Xinference自带的图形化聊天界面。</p><p><img src="https://liuqianglong.com/content/images/mypost/ai-xinference-local-model/image-20240705172924372.png" alt="image-20240705172924372"></p><p>聊天测试。</p><p><img src="https://liuqianglong.com/content/images/mypost/ai-xinference-local-model/image-20240705172928726.png" alt="image-20240705172928726"></p><p>测试<strong>图片生成</strong>模型，启动<code>sd-trubo</code>图片生成模型，<strong>模型下载和启动的时间较长，需要多等待一会</strong>，运行大概需要12G GPU。</p><p><img src="https://liuqianglong.com/content/images/mypost/ai-xinference-local-model/image-20240705173006725.png" alt="image-20240705173006725"></p><p>启动图形化聊天界面。</p><p><img src="https://liuqianglong.com/content/images/mypost/ai-xinference-local-model/image-20240708104954508.png" alt="image-20240708104954508"></p><p>使用提示词<code>cartoon cloud</code>生成图片，设置分辨率为512<em>512，点击Generate生成图片。图片像素设置的越大，生成的时间越长，占用的GPU越多，设置1024 </em> 1024像素，大致需要占用6G GPU。</p><p><img src="https://liuqianglong.com/content/images/mypost/ai-xinference-local-model/image-20240708104917935.png" alt="image-20240708104917935"></p><p>Xinference目前无法同时运行多个大模型，在运行新的模型之前，需要停止之前的模型。</p><p>测试<strong>多模态</strong>模型，启动<code>qwen-vl-chat</code>视觉聊天模型，模型下载和启动也需要较长时间，模型需要20G GPU才能运行，所以至少需要<code>g5.xlarge</code>（24G GPU）才能运行，<code>g4dn.xlarge</code>（16G GPU）无法运行。</p><p><img src="https://liuqianglong.com/content/images/mypost/ai-xinference-local-model/image-20240829115013624.png" alt="image-20240829115013624"></p><p>上传图片聊天测试。</p><p><img src="https://liuqianglong.com/content/images/mypost/ai-xinference-local-model/image-20240829114943971.png" alt="image-20240829114943971"></p><h2 id="四、Xinference-启动嵌入和重排模型"><a href="#四、Xinference-启动嵌入和重排模型" class="headerlink" title="四、Xinference 启动嵌入和重排模型"></a>四、Xinference 启动嵌入和重排模型</h2><p>Xinference只能同时启动一个语音模型、图片模型、语音模型，但是可以同时启动多个嵌入模型、重排模型。</p><p>这里使用的嵌入（embedding）模型是<code>bge-m3</code>，重排（reranker）模型是<code>bge-reranker-v2-m3</code>。</p><p><img src="https://liuqianglong.com/content/images/mypost/ai-xinference-local-model/image-20240708144807154.png" alt="image-20240708144807154"></p><p>启动<code>bge-m3</code>嵌入模型，ollama后续可以调用这个模型。</p><p><img src="https://liuqianglong.com/content/images/mypost/ai-xinference-local-model/image-20240708132935817.png" alt="image-20240708132935817"></p><p>模型正常启动，后续Dify可以调用此嵌入模型。</p><p><img src="https://liuqianglong.com/content/images/mypost/ai-xinference-local-model/image-20240708132940587.png" alt="image-20240708132940587"></p><p>启动<code>bge-reranker-v2-m3</code>重排模型，ollama 后续可以调用这个模型。</p><p><img src="https://liuqianglong.com/content/images/mypost/ai-xinference-local-model/image-20240708132944835.png" alt="image-20240708132944835"></p><p>模型正常启动，后续Dify可以调用此重排模型。</p><p><img src="https://liuqianglong.com/content/images/mypost/ai-xinference-local-model/image-20240708133009007.png" alt="image-20240708133009007"></p><h2 id="五、文档链接"><a href="#五、文档链接" class="headerlink" title="五、文档链接"></a>五、文档链接</h2><ul><li>[1] Xinference Github主页：<a href="https://github.com/xorbitsai/inference/blob/main/README_zh_CN.md?ref=liuqianglong.com">https://github.com/xorbitsai/inference/blob/main/README_zh_CN.md</a></li><li>[2] Xinference 环境变量：<a href="https://inference.readthedocs.io/zh-cn/latest/getting_started/environments.html?ref=liuqianglong.com">https://inference.readthedocs.io/zh-cn/latest/getting_started/environments.html</a></li><li>[3] Xinference Docker安装文档：<a href="https://inference.readthedocs.io/zh-cn/latest/getting_started/using_docker_image.html?ref=liuqianglong.com">https://inference.readthedocs.io/zh-cn/latest/getting_started/using_docker_image.html</a></li><li>[4] 嵌入模型bge-m3：<a href="https://huggingface.co/BAAI/bge-m3?ref=liuqianglong.com">https://huggingface.co/BAAI/bge-m3</a></li><li>[5] 重排模型bge-reranker-v2-m3：<a href="https://huggingface.co/BAAI/bge-reranker-v2-m3?ref=liuqianglong.com">https://huggingface.co/BAAI/bge-reranker-v2-m3</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Xinference-本地运行大模型&quot;&gt;&lt;a href=&quot;#Xinference-本地运行大模型&quot; class=&quot;headerlink&quot; title=&quot;Xinference 本地运行大模型&quot;&gt;&lt;/a&gt;Xinference 本地运行大模型&lt;/h1&gt;&lt;p&gt;&lt;img s</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>安装Dify并集成Ollama和Xinference</title>
    <link href="https://xtaiyang.github.io/posts/1126c401.html"/>
    <id>https://xtaiyang.github.io/posts/1126c401.html</id>
    <published>2025-02-06T11:25:38.000Z</published>
    <updated>2025-03-11T08:14:12.507Z</updated>
    
    <content type="html"><![CDATA[<h1 id="安装-Dify-并集成-Ollama-和-Xinference"><a href="#安装-Dify-并集成-Ollama-和-Xinference" class="headerlink" title="安装 Dify 并集成 Ollama 和 Xinference"></a>安装 Dify 并集成 Ollama 和 Xinference</h1><p><img src="https://liuqianglong.com/content/images/size/w2000/2024/09/ai-dify-ollama-xinference.png" alt="安装 Dify 并集成 Ollama 和 Xinference"></p><p>本文介绍了通过 Docker 安装 Dify，然后集成 Ollama 和 XInference，并利用 Dify 快速搭建一个基于知识库问答的应用。</p><h2 id="一、Dify-简介"><a href="#一、Dify-简介" class="headerlink" title="一、Dify 简介"></a>一、Dify 简介</h2><p><strong>Dify</strong> 是一款开源的大语言模型（LLM）应用开发平台，旨在帮助开发者快速构建和部署生成式 AI 应用。以下是 Dify 的主要功能和特点 [1]：</p><ul><li><strong>融合 Backend as Service 和 LLMOps 理念</strong>：Dify 将后端即服务（Backend as Service）和 LLMOps 的理念结合，使开发者能够快速搭建生产级的生成式 AI 应用。</li><li><strong>支持多种模型</strong>：Dify 支持数百种专有和开源的 LLM 模型，包括 GPT、Mistral、Llama3 等，能够无缝集成来自多家推理提供商和自托管解决方案的模型。</li><li><strong>直观的 Prompt 编排界面</strong>：Dify 提供了一个直观的 Prompt IDE，用于编写提示、比较模型性能，并向基于聊天的应用程序添加语音转换等附加功能。</li><li><strong>高质量的 RAG 引擎</strong>：Dify 拥有广泛的 RAG 功能，涵盖从文档摄取到检索的一切，并支持从 PDF、PPT 等常见文档格式中提取文本。</li><li><strong>集成 Agent 框架</strong>：用户可以基于 LLM 函数调用或 ReAct 定义代理，并为代理添加预构建或自定义工具。Dify 提供了 50 多种内置工具，如 Google 搜索、DELL·E、Stable Diffusion 和 WolframAlpha。</li><li><strong>灵活的流程编排</strong>：Dify 提供了一个强大的可视化画布，用于构建和测试强大的 AI 工作流，使开发者可以直观地设计和优化他们的 AI 流程。</li><li><strong>全面的监控和分析工具</strong>：Dify 提供了监控和分析应用日志和性能的工具，开发者可以根据生产数据和注释不断改进提示、数据集和模型。</li><li><strong>后端即服务</strong>：Dify 的所有功能都附带相应的 API，因此可以轻松将 Dify 集成到您自己的业务逻辑中。</li></ul><h2 id="二、Dify-安装"><a href="#二、Dify-安装" class="headerlink" title="二、Dify 安装"></a>二、Dify 安装</h2><p>拷贝 Dify Github代码到本地 [2]。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/langgenius/dify.git</span><br></pre></td></tr></table></figure><p>Copy</p><p>进入 dify 源代码的 docker 目录，拷贝环境变量。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd dify/docker</span><br><span class="line">cp .env.example .env</span><br></pre></td></tr></table></figure><p>Copy</p><p>通过docker compose安装应用。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker compose up -d</span><br></pre></td></tr></table></figure><p>进入ollama容器，启动<code>qwen2:7b</code>模型。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">root@ip-172-31-30-167:~/dify/docker# docker pull ollama/ollama</span><br><span class="line">root@ip-172-31-83-158:~/dify/docker# docker run -d --gpus=all -v ollama:/root/.ollama -p 11434:11434 --name ollama --restart always -e OLLAMA_KEEP_ALIVE=-1 ollama/ollama</span><br><span class="line">root@ip-172-31-83-158:~/dify/docker# docker exec -it ollama bash</span><br><span class="line">root@b094349fc98c:/# ollama run qwen2:7b</span><br></pre></td></tr></table></figure><h2 id="三、Dify-添加Ollama模型问答"><a href="#三、Dify-添加Ollama模型问答" class="headerlink" title="三、Dify 添加Ollama模型问答"></a>三、Dify 添加Ollama模型问答</h2><p>通过EC2的公网IP地址加上80端口，登录Dify主页，创建管理账户。</p><p><img src="https://liuqianglong.com/content/images/mypost/ai-dify-ollama-xinference/image-20240831210619928.png" alt="image-20240831210619928"></p><p>通过管理员账号登录。</p><p><img src="https://liuqianglong.com/content/images/mypost/ai-dify-ollama-xinference/image-20240831210856556.png" alt="image-20240831210856556"></p><p>点击用户-设置。<img src="https://liuqianglong.com/content/images/mypost/ai-dify-ollama-xinference/image-20240912155736438.png" alt="image-20240912155736438"></p><p>添加Ollama模型。<img src="https://liuqianglong.com/content/images/mypost/ai-dify-ollama-xinference/image-20240831210928284.png" alt="image-20240831210928284"></p><p>添加<code>qwen2:7b</code>模型，因为Ollama是在本机启动，所以设置URL为本地IP地址，端口为<code>114341</code>，</p><blockquote><p>qwen2-7b-instruct 利用YARN（一种增强模型长度外推的技术）支持 131,072 tokens上下文，为了保障正常使用和正常输出，建议API限定用户输入为 128,000 ，输出最大 6,144。[3]</p></blockquote><p><img src="https://liuqianglong.com/content/images/mypost/ai-dify-ollama-xinference/image-20240831210956971.png" alt="image-20240831210956971"></p><p>点击 工作室-创建空白应用</p><p><img src="https://liuqianglong.com/content/images/mypost/ai-dify-ollama-xinference/image-20240831211013899.png" alt="image-20240831211013899"></p><p>创建“聊天助手”类型的应用，设置应用名称为<code>Qwen2-7B</code>，点击创建。</p><p><img src="https://liuqianglong.com/content/images/mypost/ai-dify-ollama-xinference/image-20240831211016746.png" alt="image-20240831211016746"></p><p>为应用设置提示词”你是一个人工智能助手”，可以和<code>Qwen2:7B</code>进行对话测试，这里是和大模型本身进行对话，没有引入外部的知识库，后续会引入知识库比较回答的结果。</p><p><img src="https://liuqianglong.com/content/images/mypost/ai-dify-ollama-xinference/image-20240831211022033.png" alt="image-20240831211022033"></p><h2 id="四、Dify-基于知识库问答"><a href="#四、Dify-基于知识库问答" class="headerlink" title="四、Dify 基于知识库问答"></a>四、Dify 基于知识库问答</h2><p>添加<code>Xorbits Inference</code>提供的模型。</p><p><img src="https://liuqianglong.com/content/images/mypost/ai-dify-ollama-xinference/image-20240831224623367.png" alt="image-20240708133836384"></p><p>添加<code>Text Embedding</code>，即文本嵌入模型，模型的名称为<code>bge-m3</code>，服务器URL为<code>http://172.31.30.167:9997</code>（这里是本机的IP，也可以安装在其他机器，网络和端口可达即可），已经提前在本机上启动了XInference，并且启动了<code>bge-m3</code>模型（参考上一篇文章）。</p><p><img src="https://liuqianglong.com/content/images/mypost/ai-dify-ollama-xinference/image-20240831224701205.png" alt="image-20240831224701205"></p><p>添加<code>Rerank</code>，即重排模型，模型的名称为<code>bge-reraker-v2-m3</code>，服务器URL为<code>http://172.31.30.167:9997</code>（这里是本机的IP，也可以安装在其他机器，网络和端口可达即可），已经提前在本机上启动了XInference，并且启动了<code>bge-reraker-v2-m3</code>模型（参考上一篇文章）。</p><p><img src="https://liuqianglong.com/content/images/mypost/ai-dify-ollama-xinference/image-20240831224932686.png" alt="image-20240831224932686"></p><p>查看系统默认设置。<img src="https://liuqianglong.com/content/images/mypost/ai-dify-ollama-xinference/image-20240831225022229.png" alt="image-20240831225022229"></p><p>点击“知识库”-“导入已有文本”-“上传文本文件”-选择《促进和规范数据跨境流动规定》的文档。<img src="https://liuqianglong.com/content/images/mypost/ai-dify-ollama-xinference/image-20240831225042306.png" alt="image-20240831225042306"></p><p>导入成功后，设置文本检索方式，开启<code>Rerank</code>模型，选择<code>bge-reranker-v2-m3</code>模型，开启默认的<code>Score</code>阈值为0.5（即文本匹配度低于0.5分时，不会召回，不会添加到大模型的上下文中）。<img src="https://liuqianglong.com/content/images/mypost/ai-dify-ollama-xinference/image-20240831225141554.png" alt="image-20240831225141554"></p><p>在之前的聊天应用中，添加上面创建的知识库，重新询问大模型相同的问题，可以看到模型结合知识库进行了回答。<img src="https://liuqianglong.com/content/images/mypost/ai-dify-ollama-xinference/image-20240831225356966.png" alt="image-20240831225356966"></p><p>可以点击“Prompt日志”，查看日志文件，可以查看系统提示词，将匹配的知识库内容放在了<code>&lt;context&gt;&lt;/context&gt;</code>中。</p><p><img src="https://liuqianglong.com/content/images/mypost/ai-dify-ollama-xinference/image-20240831231810412.png" alt="image-20240831231810412"></p><p>点击创建的知识库-点击“召回测试”，可以输入一段文本，用与匹配知识库中的文本，匹配到的文本有一个权重分数，上面设置过的阈值是0.5，即大于这个分数的才会显示为“召回段落”。</p><p><img src="https://liuqianglong.com/content/images/mypost/ai-dify-ollama-xinference/image-20240831231815155.png" alt="image-20240831231815155"></p><h2 id="五、文档链接"><a href="#五、文档链接" class="headerlink" title="五、文档链接"></a>五、文档链接</h2><ul><li>[1] Dify 官网：<a href="https://dify.ai/zh?ref=liuqianglong.com">https://dify.ai/zh</a></li><li>[2] Dify Docker Compose 部署：<a href="https://docs.dify.ai/v/zh-hans/getting-started/install-self-hosted/docker-compose?ref=liuqianglong.com">https://docs.dify.ai/v/zh-hans/getting-started/install-self-hosted/docker-compose</a></li><li>[3] Qwen Token限制：<a href="https://help.aliyun.com/zh/dashscope/developer-reference/tongyi-qianwen-7b-14b-72b-api-detailes?ref=liuqianglong.com">https://help.aliyun.com/zh/dashscope/developer-reference/tongyi-qianwen-7b-14b-72b-api-detailes</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;安装-Dify-并集成-Ollama-和-Xinference&quot;&gt;&lt;a href=&quot;#安装-Dify-并集成-Ollama-和-Xinference&quot; class=&quot;headerlink&quot; title=&quot;安装 Dify 并集成 Ollama 和 Xinferenc</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>最新OnlyFans订阅指南：OnlyFans怎样玩？中国人怎么用OnlyFans？</title>
    <link href="https://xtaiyang.github.io/posts/80a3a73c.html"/>
    <id>https://xtaiyang.github.io/posts/80a3a73c.html</id>
    <published>2025-01-20T07:31:50.000Z</published>
    <updated>2025-03-11T08:14:12.504Z</updated>
    
    <content type="html"><![CDATA[<h2 id="什么是OnlyFans？OnlyFans官网是哪个？"><a href="#什么是OnlyFans？OnlyFans官网是哪个？" class="headerlink" title="什么是OnlyFans？OnlyFans官网是哪个？"></a>什么是OnlyFans？OnlyFans官网是哪个？</h2><p>OnlyFans是一个基于订阅制的内容分享平台，成立于2016年，总部位于英国伦敦。平台的初衷是为内容创作者提供一个直接从粉丝获取收入的渠道，通过订阅费用或小费支持他们的创作。这种模式打破了传统社交媒体依赖广告收入的局限，为创作者提供了更高的收入自主性。</p><p>OnlyFans最初吸引的创作者以健身教练、音乐人和摄影师为主，平台也提供多样化的内容，包括音乐、健身课程、摄影作品等。然而，随着发展，成人内容逐渐成为平台的主要特色，吸引了大量专注于此类创作的用户和消费者。特别是在疫情期间，由于居家隔离导致线上内容需求激增，OnlyFans用户和创作者数量迅速增长，注册用户已超过2亿，创作者已超过200万。</p><p>尽管取得了巨大成功，OnlyFans在发展过程中也面临不少挑战。2021年，由于支付渠道对成人内容的限制，平台一度计划禁止此类内容，但在用户和创作者的强烈反对下，这一决定被迅速撤回。此外，平台还需应对来自其他订阅制平台的竞争，以及内容隐私和创作者权益保护等问题。为此，OnlyFans加强了内容审核机制，并推出多项隐私保护措施。</p><p>OnlyFans吸引人的地方在于其内容的多样性和独特性。用户可以通过付费订阅访问创作者的独家内容，还可以与创作者直接互动，这种紧密的互动体验是许多传统社交平台无法提供的。此外，创作者享有极高的自主权，可以自由设定内容价格并管理粉丝关系，这也使得平台成为许多人的主要收入来源。</p><p>通过独特的模式和强大的用户粘性，OnlyFans已经成为一个备受关注的内容平台，不仅改变了创作者与粉丝的互动方式，还引领了订阅制经济的新潮流。</p><p>虽然OnlyFans目前在中国已经解封，但平台的使用仍然存在一定限制。例如，你可以访问OnlyFans官网，但如果没有关注任何博主，所能看到的内容仅限于平台官方推送的信息。OnlyFans的内置搜索功能并不支持直接查找新博主或特定内容，搜索结果仅限于你已关注的博主列表。因此，如果想浏览其他创作者的内容，必须事先获取到该博主的OnlyFans主页地址。关于如何解决这一问题，请参考本文的后续部分。</p><p>同时需要提醒的是，即使某些OnlyFans博主的主页支持免费订阅，国内用户在观看内容之前仍需绑定一张信用卡。未绑定信用卡的账户将无法正常访问，即便是免费的订阅内容也不例外。</p><h2 id="中国人怎么用OnlyFans？"><a href="#中国人怎么用OnlyFans？" class="headerlink" title="中国人怎么用OnlyFans？"></a>中国人怎么用OnlyFans？</h2><p>OnlyFans并未开发独立的APP客户端，全球用户只能通过OnlyFans官网进行注册和访问。而自2024年11月29日起，OnlyFans已经解除封锁，中国用户无需使用科学上网工具即可直接访问官网。此外，OnlyFans对中国用户较为友好，支持使用QQ邮箱和163邮箱进行注册。按照本文提供的步骤操作，你就可以轻松注册OnlyFans账号，并自由使用平台的各项功能了。</p><h2 id="如何注册OnlyFans帐号？"><a href="#如何注册OnlyFans帐号？" class="headerlink" title="如何注册OnlyFans帐号？"></a>如何注册OnlyFans帐号？</h2><p>注册OnlyFans账号非常简单，只需一个邮箱即可完成。而且OnlyFans对中国用户相对友好，支持使用国内的QQ邮箱和网易163邮箱进行注册。当然，如果你拥有Google账号或Twitter账号，也可以选择通过Google帐号或Twitter账号快速注册OnlyFans账号。注册onlyfans帐号操作步骤如下：</p><h3 id="第一步：访问OnlyFans官网并点击注册。"><a href="#第一步：访问OnlyFans官网并点击注册。" class="headerlink" title="第一步：访问OnlyFans官网并点击注册。"></a>第一步：访问OnlyFans官网并点击注册。</h3><ul><li>OnlyFans官网地址：onlyfans.com</li></ul><h3 id="第二步：填写注册所需的个人信息，包括邮箱、用户名和密码等，确保信息准确无误。"><a href="#第二步：填写注册所需的个人信息，包括邮箱、用户名和密码等，确保信息准确无误。" class="headerlink" title="第二步：填写注册所需的个人信息，包括邮箱、用户名和密码等，确保信息准确无误。"></a>第二步：填写注册所需的个人信息，包括邮箱、用户名和密码等，确保信息准确无误。</h3><img src="/posts/80a3a73c/c5febaa2c8c140db80157e1b1262229a.png" class="" title="图片描述"><!-- ![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/c5febaa2c8c140db80157e1b1262229a.png) --><h3 id="第三步：完成邮箱验证。"><a href="#第三步：完成邮箱验证。" class="headerlink" title="第三步：完成邮箱验证。"></a>第三步：完成邮箱验证。</h3><p>注意：系统会向你填写的邮箱地址发送验证邮件，点击邮件中的验证链接即可成功完成账号注册。</p><img src="/posts/80a3a73c/6d493d3e0e14433fbb6e7a34b94e53cc.png" class="" title="图片描述"><!-- ![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/6d493d3e0e14433fbb6e7a34b94e53cc.png) --><p>恭喜你，截止到目前为止，你已经成功完成了OnlyFans账号的注册。需要再次提醒的是，即使某些OnlyFans博主的主页支持免费订阅，用户仍需绑定一张信用卡后才能正常观看内容。如果未绑定信用卡，可能无法查看任何内容。</p><h2 id="OnlyFans充值订阅教程：OnlyFans怎样玩？"><a href="#OnlyFans充值订阅教程：OnlyFans怎样玩？" class="headerlink" title="OnlyFans充值订阅教程：OnlyFans怎样玩？"></a>OnlyFans充值订阅教程：OnlyFans怎样玩？</h2><p>OnlyFans是近年来最火爆的国外私人社交平台之一，但如果想观看OnlyFans上某位博主分享的内容，就必须先进行订阅。然而，非常遗憾的是，OnlyFans目前仅支持国外信用卡订阅，这意味着国内的双币Visa卡或其他类型的Visa卡都无法用于订阅OnlyFans。</p><p>注意：许多新手用户喜欢在某宝上搜索“OnlyFans充值”“OnlyFans支付”“OnlyFans开通”等关键词，然后通过某宝购买虚拟信用卡。这种方式实际上非常危险，并不推荐。某宝上的价格通常很高，售后服务没有保障，购买后就没人管了。更重要的是，某宝上许多虚拟信用卡都是一次性使用的，许多新手购买后发现无法继续使用，最终哭晕在了厕所。</p><p>那么，那么Onlyfans怎样玩呢？大致可以分为三个步骤：</p><ul><li>1，获取一张虚拟信用卡</li><li>2，为OnlyFans绑定信用卡 </li><li>3，订阅你喜欢的OnlyFans博主</li></ul><p>但是，不管什么方式，都不建议购买充值，风险很大需谨慎！！！</p><h2 id="如何寻找OnlyFans博主？"><a href="#如何寻找OnlyFans博主？" class="headerlink" title="如何寻找OnlyFans博主？"></a>如何寻找OnlyFans博主？</h2><p>OnlyFans的新账号默认是看不到任何内容的，同时你也无法通过OnlyFans内置的搜索功能查找你喜欢的内容。如果想寻找OnlyFans博主，我们为大家推荐两种简单有效的方法：</p><p>方法一：通过Twitter或某站等平台进行搜索，找到你喜欢的博主的OnlyFans主页链接。许多博主会在Twitter或P站的个人资料中分享自己的OnlyFans主页链接，你可以通过这些平台查找并直接跳转到博主的主页进行订阅。</p><p>方法二：使用第三方工具“OnlySearch”进行搜索。如果你已经知道某位OnlyFans博主的名字，可以在OnlySearch中输入该名字，快速找到相关搜索结果，从中找到博主的OnlyFans主页链接。</p><p>OK，以上两种方式都非常高效且便捷。第一种方式适合寻找你尚不了解具体名字的OnlyFans博主，通过探索Twitter或某站发现更多可能感兴趣的创作者。第二种方式适合已经知道名字的博主，帮助你快速定位主页链接。大家可以根据自己的需求选择适合的方式，轻松找到想订阅的OnlyFans博主。</p><h2 id="如何取消OnlyFans订阅？"><a href="#如何取消OnlyFans订阅？" class="headerlink" title="如何取消OnlyFans订阅？"></a>如何取消OnlyFans订阅？</h2><p>在订阅OnlyFans博主后，你可能会因各种原因决定取消订阅。无论是对内容不满意，还是为了控制开支，取消订阅的过程其实并不复杂。不过需要注意的是，如果你不主动取消订阅，系统会默认自动续费，这可能导致不必要的支出。以下是取消OnlyFans订阅的具体步骤：</p><ul><li><p>登录你的OnlyFans账户：打开OnlyFans官网并登录你的账号，确保使用的是与你订阅记录相关联的账户。</p></li><li><p>进入订阅管理页面：登录后，点击右上角的头像图标，进入“订阅”或“我的订阅”页面。在这里，你可以查看所有已订阅的博主。</p></li><li><p>找到需要取消订阅的博主：在订阅列表中，找到你想取消订阅的博主，点击博主的头像或名字进入其主页。</p></li><li><p>取消订阅：在博主的主页上，你会看到一个标注“已订阅”的按钮。点击该按钮后，系统会提示是否取消订阅，选择“取消订阅”即可。</p></li><li><p>确认取消：系统可能会弹出提示，询问你取消订阅的原因。这通常是可选的，你可以填写原因，也可以直接跳过。完成确认后，订阅即被取消。</p></li></ul><p>总体来说，取消OnlyFans订阅并不复杂，但需要你手动操作以避免自动续费的情况。通过以上步骤，你可以轻松管理自己的订阅列表，按需调整支出和关注的内容。记住，只有合理管理订阅，才能让你的OnlyFans使用体验更加高效和愉快！</p><h2 id="中国用户订阅和使用OnlyFans时的常见问题"><a href="#中国用户订阅和使用OnlyFans时的常见问题" class="headerlink" title="中国用户订阅和使用OnlyFans时的常见问题"></a>中国用户订阅和使用OnlyFans时的常见问题</h2><p>在中国大陆使用OnlyFans时，许多用户可能会遇到各种疑问和问题，比如能否直接访问OnlyFans官网、平台支持哪些支付方式，以及注册账号或绑定信用卡时为何会出现问题。此外，首页内容为空、无法接收验证码或需要进行年龄验证等情况也让不少用户感到困惑。针对这些常见问题，我们将逐一为你解答，帮助你更顺利地订阅和使用OnlyFans。</p><h3 id="中国大陆用户可以直接访问OnlyFans官网吗？"><a href="#中国大陆用户可以直接访问OnlyFans官网吗？" class="headerlink" title="中国大陆用户可以直接访问OnlyFans官网吗？"></a>中国大陆用户可以直接访问OnlyFans官网吗？</h3><p>以前中国大陆用户无法直接访问OnlyFans官网，但自2024年11月29日起，中国大陆用户已经可以自由访问OnlyFans官网，无需使用任何特殊工具。</p><h3 id="OnlyFans支持哪些支付方式？"><a href="#OnlyFans支持哪些支付方式？" class="headerlink" title="OnlyFans支持哪些支付方式？"></a>OnlyFans支持哪些支付方式？</h3><p>OnlyFans目前主要支持国际信用卡和借记卡支付，包括Visa和MasterCard等。然而，对于中国大陆用户来说，国内发行的信用卡和借记卡通常无法在OnlyFans上完成支付。此外，支付宝和微信支付等国内常用的支付方式也不被支持。因此，中国用户必须选择使用虚拟信用卡服务，才能成功订阅OnlyFans。 </p><h3 id="为什么Onlyfans的首页啥也没有？"><a href="#为什么Onlyfans的首页啥也没有？" class="headerlink" title="为什么Onlyfans的首页啥也没有？"></a>为什么Onlyfans的首页啥也没有？</h3><p>OnlyFans的首页默认展示平台推荐的内容，但如果你刚注册账号且未关注任何博主，首页会显示为空。这是因为OnlyFans的内容推荐主要基于你已关注的博主，并不会主动展示未关注博主的内容。如果想在首页看到更多内容，你需要先获取博主的主页链接并订阅，之后首页才会显示相关内容。 </p><h3 id="注册Onlyfans时收不到验证码怎么办？"><a href="#注册Onlyfans时收不到验证码怎么办？" class="headerlink" title="注册Onlyfans时收不到验证码怎么办？"></a>注册Onlyfans时收不到验证码怎么办？</h3><p>如果在注册OnlyFans时收不到验证码，可以尝试以下方法。首先，确保注册时填写的邮箱地址正确，避免因输入错误导致无法接收验证码。其次，检查邮箱的垃圾邮件文件夹，因为验证码邮件可能被误判为垃圾邮件。此外，有时邮件可能会有延迟，建议稍等几分钟再刷新邮箱查看。如果仍未收到，可以在注册页面点击“重新发送验证码”按钮，确保邮件再次发送到你的邮箱。同时，检查你的网络连接是否正常，以避免注册过程中的延迟或错误。</p><p>如果你使用的是国内邮箱（如QQ邮箱或163邮箱），可能因为邮件服务商的限制导致无法接收验证码。在这种情况下，建议尝试使用Gmail或Outlook等国际邮箱重新注册。</p><p>如果以上方法仍然无法解决问题，可以联系OnlyFans的客服团队，寻求进一步的帮助和支持。</p><h3 id="Onlyfans需要验证年龄怎么办？"><a href="#Onlyfans需要验证年龄怎么办？" class="headerlink" title="Onlyfans需要验证年龄怎么办？"></a>Onlyfans需要验证年龄怎么办？</h3><p>OnlyFans要求用户验证年龄是为了符合法律规定，确保未成年人无法接触平台上的内容。这一机制既保障用户的自由，又履行了社会责任。由于平台内容多样，OnlyFans规定只有达到法定年龄的用户才能访问，以符合未成年人保护的相关法律。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;什么是OnlyFans？OnlyFans官网是哪个？&quot;&gt;&lt;a href=&quot;#什么是OnlyFans？OnlyFans官网是哪个？&quot; class=&quot;headerlink&quot; title=&quot;什么是OnlyFans？OnlyFans官网是哪个？&quot;&gt;&lt;/a&gt;什么是OnlyF</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>互联网公司技术岗实习/求职经验（实习内推+简历+面试+offer篇）</title>
    <link href="https://xtaiyang.github.io/posts/b5bab9.html"/>
    <id>https://xtaiyang.github.io/posts/b5bab9.html</id>
    <published>2024-11-13T13:27:14.000Z</published>
    <updated>2025-03-11T08:14:12.499Z</updated>
    
    <content type="html"><![CDATA[<h1 id="互联网公司技术岗实习-求职经验（实习内推-简历-面试-offer篇）"><a href="#互联网公司技术岗实习-求职经验（实习内推-简历-面试-offer篇）" class="headerlink" title="互联网公司技术岗实习/求职经验（实习内推+简历+面试+offer篇）"></a><a href="https://www.cnblogs.com/joyeecheung/p/5003980.html">互联网公司技术岗实习/求职经验（实习内推+简历+面试+offer篇）</a></h1><p>找工作的事基本尘埃落定了，打算把这大半年来积累的经验写下来，基本都是我希望当年找实习的时候自己能够知道的东西，帮师弟师妹们消除一点信息不平等，攒攒RP~ 不要像我当年那样，接到电话吓成狗，没接到电话吓成狗，一点小事就惶惶不可终日，面试完疑神疑鬼啥的……这些大部分都是像我一样经历过大半年实习求职经历后就会知道的常识，但是还在学校的师弟师妹们就未必知道了，所以感觉还是有必要写写，给后人栽个树好乘凉。</p><p>因为个人经历的缘故，会多讲一些阿里相关的东西，另外有一些东西是跟其他实习生交流过之后了解到的，也一并写下来。另外因为有一次比较特殊的实习转正时内部转岗的经历，当时在网上几乎没找到多少相关信息，以后有空也会写写过程帮助后来人了解一下。</p><p><strong>事先写一句：这篇东西不是什么应聘宝典/秘籍之类的，只是介绍一下一般在校生不了解的相关信息而已。我一直信奉的原则是，应聘是双向选择的过程，现在互联网的技术岗位对于有能力的人一直处于供不应求的状态，所以只要你有干货，技术栈有合拍的地方，自会有赏识你的人，只要确保自己的亮点不被埋没就好了，没必要生捏硬造出什么亮点出来，最后更可能坑的是自己。</strong></p><h2 id="关于实习招聘"><a href="#关于实习招聘" class="headerlink" title="关于实习招聘"></a>关于实习招聘</h2><p>进互联网公司实习，主要途径分为内推和校招两种。内推也就是公司员工把你的信息录入到内部系统进行推荐，校招则是公司来学校开宣讲会或者通过其他方式宣传这次招聘，然后你再去公司举办的招聘会（一般开在酒店）面试。</p><p>一般来说，招聘流程如下：</p><ul><li>内推：员工录入你的信息到公司招聘系统 -&gt; 筛选简历 -&gt; 面试 -&gt; 发 offer</li><li>校招：通过宣讲会或者网申等方式投递简历 -&gt; 筛选简历 -&gt; 笔试 -&gt; 面试 -&gt; 发 offer</li></ul><p>一般来说，内推可以跳过笔试，并且省去很多流程上的麻烦，而且内推一般先于校招开始，机会更多，所以能内推的最好尽量内推。这年头内推非常容易，因为如果被推荐的人通过面试，推荐他的人会得到公司的奖励（或者推荐量到一定程度有奖励），所以员工大多数也比较乐意发内推。只要你有认识的人在你感兴趣的公司工作，都可以去问问。就算没有认识的人，知乎、微博、V2EX、高校bbs等地都可以比较容易地找到各家公司的员工来收内推的信息（貌似有不少人以为推荐人和被推荐人一定要认识……其实很多时候推荐人并不在意他不认识你，因为他推进去之后你的简历还是会被筛一遍的）。</p><p>各大互联网公司的实习内推一般在 2 月底 ~ 4 月，校招在 4 月 ~ 6 月，如果坑没填够还会有补录。内推和校招的申请 deadline 一般是公司定的，过了 deadline 可能 leader 也没有太多办法（当然这不是绝对的，要看情况），所以最好在 3 月左右就开始多刷刷各种渠道，不要误了 deadline，生出不必要的麻烦。</p><h2 id="关于内推"><a href="#关于内推" class="headerlink" title="关于内推"></a>关于内推</h2><p>一般来说，大一点的公司会有自己的招聘系统，内推的时候员工一般会要你的简历，然后他再将一些信息填写进这个招聘系统。如果你有明确想去的团队，或者你准备去的就是推荐人所在的团队，就会指定相关的人来负责，处理会快一些。如果没有，那么一般是推到一个事业群或者大部门，然后想招人的 leader 来看看，遇到感兴趣的简历就捡走，这个过程可能会稍微长一些。</p><p>注意大公司一般有很多事业群/部门，一般内推了其中一个就不能再推第二个了（举个例子，阿里有淘宝、天猫、阿里云、B2B、蚂蚁金服等等多个 BU，你内推了其中一个就不能推另一个。腾讯和百度也有类似的锁定简历的情况），通常这是系统定死的，如果想改会很麻烦，所以内推前一定要慎重（我就被坑过……）。主要是大公司内各个团队其实招人时处于类似竞争的关系，一个团队看中的人当然不能随便又被另一个团队抢走。</p><h2 id="关于简历"><a href="#关于简历" class="headerlink" title="关于简历"></a>关于简历</h2><p>看你简历的人一般有两种：HR 或者要招人的团队 leader，所以在制作简历的时候两种人都要考虑到。对于 HR 来说，联系方式、教育经历比较重要。对于团队 leader 来说，项目经验、技能、获奖情况比较重要（一些团队还会看重 GitHub 之类的公开信息）。</p><p>怎样写一个好的简历已经有非常多的讨论，这里我就不啰嗦了，只说一些个人经验：</p><ol><li>通常来说面试官不会花很多时间看你的简历，而且很多是面试的时候再看的，所以重点一定要突出。比如依照<a href="https://en.wikipedia.org/wiki/Golden_Triangle_(Internet_Marketing">黄金三角</a>)理论，把你最想让他看到的东西放在左上角，以及运用加粗、颜色对比、字体等方式强调你想强调的东西。根据个人经验，放到简历下面的东西有可能直到面试结束他都不会看一眼（T_T）。</li><li>简历格式最好是 PDF，如果有在线简历的话更方便传播（GitHub Pages 什么的弄一个很快哒），而且你还可以用 Google Analytics 之类的工具分析你的简历浏览情况哦~（比如你看到有很多来自杭州的访问量的时候，就知道有很大几率被阿里的人看到了） 另外个别公司现场面还是需要纸质简历的，如果你不打算彩色打印但是简历又是彩色的话，要记得考虑一下简历转换成黑白之后的打印效果（比如淡色背景可能打出来一片糊！= =）</li><li>简历越早准备越好（我是春节准备的），多参考一下别人的简历，如果对设计感兴趣的可以去 dribbble 之类的地方搜搜 resume 之类的关键词，看多了就知道哪些该做哪些不该做了。</li></ol><p>另外因为参与过一次内推，说一下看到的一些雷区：</p><ol><li>不要写和你的岗位完全无关的经验……虽然不是每个面试官都反感，不过如果没什么亮点的话他会觉得你没有干货，写这些纯在凑数。</li><li>简历的外观和信息排布虽然能帮助你的能力得到更好的展现，但是最重要的是你的能力，不是你的简历，不要本末倒置……如果没有干货，简历再好看，懂行的人也能识破的。</li></ol><h2 id="关于面试"><a href="#关于面试" class="headerlink" title="关于面试"></a>关于面试</h2><p>如果你面的公司不在你所在的城市，一般会先电面。电面可能是打电话，也可能是 Skype、QQ（对，腾讯的……）、旺旺（阿里的）之类。一般有两种情况：HR打来约时间，或者是一面面试官直接打过来。后者的话可能先问问你有没有时间直接开始，没有的话再约（这种情况很正常，不用担心）。</p><p>有些公司会在电面之后希望能够现场面，需要你去他所在的城市。有些公司可能全程电面（比如阿里的内推）。一般内推的会有电面，而走校招的一般直接现场面。电面的话来来去去约双方有空的时间，可能整个流程会很久（两三个星期），而现场面可能一天就都面完了……</p><p>一般来说，技术岗面试会有 2 ~ 3 面或以上。就互联网公司来说，最常见的情况是：一面面试官通常是你进去之后的导师，二面面试官是你进去之后的直接主管，三面面试官是主管的主管。另外还可能有交叉面，也就是其他部门的 leader 来面你，重复检验一下你的能力。即使不是这种排序，基本来说，面试你的人也是你未来的同事，少说也是一个大部门的，很有可能是你进去之后就工位在你方圆几十米以内。另外这个安排不是绝对的，因为你的同事都是干活的程序员，可不是专职搞招聘的，可能招人的时候他们正好特别忙，会拉其他团队的人帮手看看之类的，也就会出现传说中的面试的时候遇到了貌似根本不懂你这个领域的面试官……</p><p>这几面基本都是问的技术问题，一面是你的直接同事所以会问的比较细比较基础，二面开始就没那么细节了，因为主管们很多都已经不直接敲代码而是负责架构、把握整体技术方向，所以更多是问的一些理解和经验，主要是开放式问题。还有，由于面试官通常是你的未来同事，所以肯定多少会注意你本人是不是特别奇葩，毕竟大家以后低头不见抬头见啊……另外大公司都会有 HR 面，主要是看看你性格正不正常之类的。</p><p>一般公司的 HR 只是提供建议供面试官参考，但是阿里的 HR 面比较特殊。阿里有一种叫 HRG 的职位，俗称“政委”，HR 面的时候面试官就是这个团队的 HRG，在招人的时候是有一票否决权的。一般 HRG 主要看看你是不是有所谓的“阿里味”，符不符合阿里的价值观，想知道是啥的可以搜索“阿里 六脉神剑”。虽然在知乎上 HRG 被描述得很恐怖，不过就个人经历过的四个 HRG 来看，好像并没有什么特别的，都是正常的大哥哥大姐姐……HR 面的时候也跟其他公司的 HR 问的差不多，基本就是有什么兴趣爱好啊，家里对工作地点有什么看法啊，觉得自己有什么优缺点啊，怎么学习技术的啊之类的问题。要说有什么特别的，大概就是我以为 HRG 不懂技术，所以回答问题的时候特意解释得外行人比较能听懂，结果说着说着发现貌似她很懂啊 0-0……包括后来我转岗的时候才知道 HRG 为了能够让我充分发挥自己的能力，在安排我的岗位的时候还是费了不少心思的，也为我的职业发展提了很多建议，作为一个孤身跑来杭州的家伙，发现在这个人生地不熟的地方有个温柔的大姐姐这么为自己着想，心里还是很感动的。</p><p>有些公司有专门的 HR 负责流程事宜，那么一般是 HR 来跟你约面试时间，如果面试官没按时跟你联络，他会帮你解决，而且会发邮件和短信提醒你面试时间。如果没有专门的 HR 负责（比如阿里= =），那么从头到尾基本都是面试官直接跟你联络。但是前面也说了面试官可能就是你的未来同事，毕竟不是专职 HR，可能组织上没有那么缜密，漏打电话或者开个会没按时联络你也是很正常的。遇到这种情况耐心等等就行了，如果时间太长再去找人（比如内推你的人、校招客服）反馈一下，免得人家是真的直接把你给忘了……</p><p>基本上所有的面试都是这种流程：自我介绍 -&gt; 问一些基础问题（可能没有）-&gt; 出题给你做（可能没有）-&gt; 依据你的简历问一些你写在上面的东西 -&gt; 你有什么要问我的吗？</p><p>如果面试官没有事先看过你的简历，那他就会在你自我介绍的时候一边听一边扫一眼简历。自我介绍嘛，如果像我这样中规中矩的就是说一下我叫XXX，在XXX大学读XXX专业，现在大X，我以前在 XXX 做过 XXX，XXX 做过 XXX……当然也有人喜欢自由发挥的，不过面试官貌似一般不会特别在意你说什么因为这段时间他们都在看简历……</p><p>接下来基本就是面试官问，你回答，然后这样答答答答好多轮……个人经验是，不懂的东西不要装懂，不然如果他越问越深你会跪的。如果他出题给你做的话，关键的是展示你思考的过程（所以最好不要冷场，就算没有瞬间想出答案，也要把你内心的自言自语说出来），结果未必是最重要的（当然答出来总好过答不出）。如果他问你的东西你答不上来，也不一定要直接表示我不懂，可以说说你了解的相关的知识。</p><p>总之面试的要点就是，你不懂的没必要硬装，但是你懂的东西一定要尽量展现给他。说到底，只要让他能够尽量全面客观地评价你这个人就够了，面试官自会判断你是不是他想招的人，如果大家不合拍，你硬靠装逼拿一个 offer 也没什么意思，就算不小心进去了，也可能感觉日狗……</p><p>面试的最后一个问题基本上都是“你还有什么要问我的吗？”（我还真没见过最后一个问题不是这个的面试官23333），网上的面经很多是教你怎样再借机表现一下自己的，不过个人觉得比较有用的问题是：</p><ol><li>我进去之后会做什么？团队是做什么东西的（业务是什么）？内部项目还是外部项目？偏基础还是偏业务（简单粗暴地说，做基础就是写给程序员用的东西，做业务就是写给用户用的东西）？技术氛围怎么样？主要用到什么技术？有什么开源产出吗？你们做 code review 吗？你们写单元测试吗？等等等等…… 这些问题是帮助你拿到 offer 之后决定要不要接的，如果你投的不止一家公司，而且到时候拿到的 offer 势均力敌，这个信息就十分有用了。</li><li><p>就我之前的表现来看，你觉得我的优缺点在哪里？这个问题可以侧面打探出他对你的评价，而且可以帮助你给自己查漏补缺。</p><p>大一点的公司一般会有个外部的招聘系统，面试结果可以在上面查到。2015年阿里招实习生的时候是可以直接看到每一面的通过情况的。</p></li></ol><h2 id="关于接-offer"><a href="#关于接-offer" class="headerlink" title="关于接 offer"></a>关于接 offer</h2><p>一般 offer 有口头和书面两种。有时候要你答应了口头 offer 才能拿到书面 offer（我遇到的腾讯 SNG 是这样）。一般来说拿了 offer 又不去的，会在正式校招（这里的校招指的是招聘应届生做正式员工）的时候再联络并且会跳过一些流程，而接了 offer 又不去的……可能会被拉黑……所以不要随便毁约，坑公司又坑自己。</p><p>（我见过对接 offer 这事最不在意的貌似是阿里，如果不接实习 offer 的话秋季校招的时候还可以走绿色通道直接进终试……）</p><p>对于选择 offer，个人的一点看法是项目/团队 &gt;　公司。特别是在大公司里，不同事业群、不同团队、不同项目，做的事情、做事风格可能天差地别。没有在大公司呆过的同学特别容易以为，大公司里很多东西是统一的……然而互联网公司很多时候隔组如隔山，你听说到的某个团队的信息，是不能随意套到另一个团队的身上的，特别是在这两个团队所属不同部门的时候，可能他们的技术栈和行事流程根本是两个次元。所以面试的时候那最后一个问题就很有用了，你可以直接把自己想问的都给问了，千万不要浪费。选择 offer 的时候，先衡量一下到底哪个团队才是你想进去做事，能让你学到东西的。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;互联网公司技术岗实习-求职经验（实习内推-简历-面试-offer篇）&quot;&gt;&lt;a href=&quot;#互联网公司技术岗实习-求职经验（实习内推-简历-面试-offer篇）&quot; class=&quot;headerlink&quot; title=&quot;互联网公司技术岗实习/求职经验（实习内推+简历+</summary>
      
    
    
    
    
    <category term="找工作" scheme="https://xtaiyang.github.io/tags/%E6%89%BE%E5%B7%A5%E4%BD%9C/"/>
    
  </entry>
  
  <entry>
    <title>注重实效的哲学程序员修炼之道第一章阅读心得</title>
    <link href="https://xtaiyang.github.io/posts/aaf337f2.html"/>
    <id>https://xtaiyang.github.io/posts/aaf337f2.html</id>
    <published>2024-11-07T02:56:37.000Z</published>
    <updated>2025-03-11T08:14:12.504Z</updated>
    
    <content type="html"><![CDATA[<h1 id="注重实效的哲学——程序员修炼之道第一章阅读心得"><a href="#注重实效的哲学——程序员修炼之道第一章阅读心得" class="headerlink" title="注重实效的哲学——程序员修炼之道第一章阅读心得"></a>注重实效的哲学——程序员修炼之道第一章阅读心得</h1><blockquote><p>强烈推荐这本书《程序员修炼之道——从小工到专家》 <a href="https://book.douban.com/subject/5387402/">https://book.douban.com/subject/5387402/</a></p></blockquote><h2 id="注重实效的程序员的特征"><a href="#注重实效的程序员的特征" class="headerlink" title="注重实效的程序员的特征"></a>注重实效的程序员的特征</h2><p>他们处理问题、寻求解决方案时的态度、风格、哲学。</p><ol><li><strong>能越出直接问题去思考更深层次的东西</strong>。总是设法把问题放到更大的语境中，设法注意更大的图景，以最小代价达到最佳效果。</li><li><strong>项目代码的维护，对自己的代码负责，将项目代码保持整洁优雅</strong>。</li><li><strong>善于发现变化、接受变化、自我改变</strong>。能克服惰性，在改变中，提高效率，自我提升。</li><li><strong>注重积累自己的知识资产</strong>。定期读好书、学新知识、温故总结。</li><li><strong>更好的和他人交流</strong>。工作生活中我们会有快一半的时间用于交流，我们应该在交流中学习，提高交流的效率，在交流中收获良师益友。</li></ol><p>以上我从书中总结的五点非常重要，下面会结合实际情况详细分析、并找到如何做的方案。</p><p>以及个人认为时间的规划也很重要，长期安排和短期安排都很重要，附 Geek 工作效率图</p><h2 id="项目代码的维护"><a href="#项目代码的维护" class="headerlink" title="项目代码的维护"></a>项目代码的维护</h2><blockquote><p>在所有弱点中。最大的弱点就是害怕暴露弱点。</p></blockquote><h3 id="软件的熵"><a href="#软件的熵" class="headerlink" title="软件的熵"></a>软件的熵</h3><p>当软件在无序增长时，也就是熵增加，软件趋向于腐烂。</p><p>但是显而易见的，有两种项目：一种随着开发，软件的熵越来越高，有着指数般的成长趋势，项目逐渐腐烂；而另一种，随着开发，软件的熵得到了很好的控制，线性般的成长趋势，甚至就好像有一个上限值一样，成长趋势越来越慢，几乎停下来。</p><p>那么是什么造成了这样的差异呢？</p><h3 id="破窗理论"><a href="#破窗理论" class="headerlink" title="破窗理论"></a>破窗理论</h3><blockquote><p>在市区，有些建筑漂亮整洁，而另一些却是破败不堪堆满垃圾。根据研究者调查发现了一种触发机制，一种很快能将整洁、完整的建筑变为破败的废弃物的机制。</p><p>破窗户</p><p>一扇破窗户，只要有那么一段时间不修理，就会给建筑的居民带来<strong>废弃感</strong>。很快有一扇窗户破了，人们开始乱扔垃圾、乱涂乱画，严重的结构损坏开始了，这个时候废弃感成为了现实，每个人都感受得到。</p><p>破窗理论启发了纽约和各大城市的警察部门，他们对一些轻微的按键严加处理，以防止大案的发生，这非常奏效。</p></blockquote><p>所以我们不要留着破窗户（低劣的设计、错误的决策、糟糕的代码）不修，发现一个就应该修一个。</p><blockquote><p>我们看到过整洁、运行良好的系统，一旦窗户开始破裂，就相当迅速的恶化。</p><p>如果你发现自己在有好些破窗户的项目里工作，会很容易产生这样的想法：“这些代码的其余部分也是垃圾，我只要照着做就行了。”项目在这之前是否一致很好，并没有任何关系。比如一辆飞起的轿车放了一星期，无人理睬，而一旦有一扇窗户被打破，数小时车上的设备就会被抢夺一空。</p><p>同样的道理，如果你发现你所在的团队和项目的代码十分漂亮——编写整洁、涉及良好并且很优雅——你就很可能会格外注意不去把它弄脏，因为你不会想成为第一个弄脏东西的人。</p></blockquote><p>所以修复破窗非常非常重要，见到一个就立马修复它。</p><h2 id="足够好的软件"><a href="#足够好的软件" class="headerlink" title="足够好的软件"></a>足够好的软件</h2><ol><li>让用户参与权衡</li><li>知道何时止步（编程就像画画，不要过度修饰和过于求精）</li></ol><h2 id="变化"><a href="#变化" class="headerlink" title="变化"></a>变化</h2><p>温水煮青蛙的事常常发生在我们生活中，随着时间的推移，任何事都会有变化，假如我们不去发现变化、并做出改变来应对，我们其实就和青蛙没有什么区别。</p><ol><li>我们要克服惰性，做变化的催化剂，用一些改变的成果(demo)给团队惊喜，和团队宣传变化后的大图景（new future），团结团队迎接变化。</li><li>变化更是挑战，正面迎战挑战，可以获得提高效率、提升自我的成果。</li></ol><h2 id="注重积累自己的知识资产"><a href="#注重积累自己的知识资产" class="headerlink" title="注重积累自己的知识资产"></a>注重积累自己的知识资产</h2><h3 id="经营管理知识资产"><a href="#经营管理知识资产" class="headerlink" title="经营管理知识资产"></a>经营管理知识资产</h3><blockquote><p>知识是有时效性的资产</p></blockquote><p>就好比一百多年前最厉害的武器 火枪大炮，在如今早已被AK、导弹所替代，更何况在当今这个每过十年都会有翻天覆地变化的时代，旧知识的价值的降低速度非常之快。</p><p>所以我们应该善于经营管理自己的知识资产，我认为更重要的是</p><ol><li>新知识和旧知识并不矛盾，应该携手共进，知识是相通的。</li><li>培养核心学习能力。学习知识的核心，而不是只学习知识的 形。</li><li>培养学习习惯、终身学习。</li></ol><p>书中的观点：</p><ol><li>定期投资</li><li>多元化</li><li>管理风险；低买高卖。</li></ol><p>最重要的是，<strong>定期为你的知识资产投资</strong></p><h3 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h3><p>制定目标并执行是一个非常好的定期投资方式。</p><p>书中举例的目标：</p><ol><li>每年至少学一种新的语言</li><li>每季度阅读一本技术书籍，也要阅读非技术书籍</li><li>学习新的思想</li><li>跟上技术潮流，</li><li>试用不同的环境、工具。</li></ol><p>我的目标：</p><ol><li>跟进社区技术发展，关注社区文章、动态</li><li>写一篇技术文章</li><li>看一本技术书籍</li><li>做一次技术分享</li><li>在技术上，给自己以新的领域的挑战</li></ol><h2 id="高效的获取知识"><a href="#高效的获取知识" class="headerlink" title="高效的获取知识"></a>高效的获取知识</h2><h3 id="学习机会"><a href="#学习机会" class="headerlink" title="学习机会"></a>学习机会</h3><ol><li>解决问题</li><li>和厉害的人一起交流自己解决不了的问题</li><li>让自己无聊的时候（排队、坐车）有东西可以学，强烈推荐 pocket</li></ol><h3 id="谷歌、问问题"><a href="#谷歌、问问题" class="headerlink" title="谷歌、问问题"></a>谷歌、问问题</h3><ol><li>首先要明确自己的问题是什么</li><li>精确地的描述问题</li><li>锲而不舍、不断思考</li></ol><h2 id="高效的交流"><a href="#高效的交流" class="headerlink" title="高效的交流"></a>高效的交流</h2><ol><li>知道你真正想要什么</li><li>了解你的听众，知道他想听什么，能听懂什么</li><li>选择好时机</li><li>选择风格：简短？正式？随意？详细？</li><li>让听众参与、做倾听者</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;注重实效的哲学——程序员修炼之道第一章阅读心得&quot;&gt;&lt;a href=&quot;#注重实效的哲学——程序员修炼之道第一章阅读心得&quot; class=&quot;headerlink&quot; title=&quot;注重实效的哲学——程序员修炼之道第一章阅读心得&quot;&gt;&lt;/a&gt;注重实效的哲学——程序员修炼之道第</summary>
      
    
    
    
    
    <category term="感悟" scheme="https://xtaiyang.github.io/tags/%E6%84%9F%E6%82%9F/"/>
    
  </entry>
  
  <entry>
    <title>让事情去发生</title>
    <link href="https://xtaiyang.github.io/posts/b5babe99.html"/>
    <id>https://xtaiyang.github.io/posts/b5babe99.html</id>
    <published>2024-11-07T02:56:37.000Z</published>
    <updated>2025-03-11T08:14:12.504Z</updated>
    
    <content type="html"><![CDATA[<h1 id="有关10000小时定律"><a href="#有关10000小时定律" class="headerlink" title="有关10000小时定律"></a>有关10000小时定律</h1><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>知乎上看到了一篇有关10000小时定律的文章写得非常好，纠正了我们以前的客观认识，并且写的很深入，记记笔记。<br><a href="https://www.zhihu.com/question/21655527/answer/44515153">https://www.zhihu.com/question/21655527/answer/44515153</a></p><p>我一直到相信一句话：</p><blockquote><p>就目前我们努力的程度，远未到拼天赋的地步。</p></blockquote><p>所以非常信奉10000小时来着，但是以前认识不深。因为我也觉得肯定有方法学的，不仅仅是10000小时，这个笔记很重要。</p><h2 id="正确认识10000小时定律"><a href="#正确认识10000小时定律" class="headerlink" title="正确认识10000小时定律"></a>正确认识10000小时定律</h2><h3 id="刻意训练"><a href="#刻意训练" class="headerlink" title="刻意训练"></a>刻意训练</h3><ol><li>需要刻意的训练。</li><li>精神动力。（刻意训练的目标，就是要让自己成为顶尖级的专家，要有为此而努力的精神动力。）</li></ol><p>有心理学家认为，人们在做决策的时候，有两个判断系统，一个是经验系统，一个是分析系统。<br>经验系统可以自主运作，轻松舒适，占用精力少;而分析系统需要你对复杂的信息作出处理，并且进行理性的分析。</p><p>经验系统不断的为分析系统提供建议：印象、直觉、意愿以及态度。如果被采纳，那么印象和直觉就会转化为信仰，灵感也会相应的转变为自主行为。</p><ol><li>刻意训练就是训练你的经验系统。</li><li>需要你以更加严格刻苦的训练来突破水平的瓶颈，也就是不断跳出你的舒适区，挑战不擅长的的区域，突破瓶颈。</li></ol><blockquote><p>keep hungry keep foolish</p></blockquote><h3 id="反馈"><a href="#反馈" class="headerlink" title="反馈"></a>反馈</h3><p>刻意训练和普通的重复性训练一个很重要的不同在于反馈。</p><p>通俗的讲就是刻意训练 <strong>需要有人的指点</strong>。</p><p>这个反馈不是表扬，而是非常明确的指出你的问题所在。只有不断发现自己的问题，然后加以改正，你才可以获得提高。</p><h3 id="深刻理解事物的内部原理"><a href="#深刻理解事物的内部原理" class="headerlink" title="深刻理解事物的内部原理"></a>深刻理解事物的内部原理</h3><p>假如你所做的事情里，<strong>存在着一目了然、始终如一的因果关系</strong>，如果你反复做这件事，并且 <strong>不断从外界获取准确反馈，改进自己表现</strong>，那么总有一天你会成为一名专家。</p><p>所以，<strong>存在一目了然、始终如一的因果关系</strong>，也就是「一万小时定律」的先决条件</p><hr><blockquote><p> 最喜欢的句子:人与事情的发展, 事情的发展，总是会涉及到很多人，因此，个人的作用其实并不那么重要。让事情去发展，才能观察态势，才能改变它，才能影响它。不要把自己看得那么重要，包括自己在内，大家都是顺势而为，不改变态势，结果早就被决定了，有你没你都一样。所以，要想赢得比赛，就得做那个能影响比赛走势的人。不然的话，胜败已定，预测的再准又有什么用？</p></blockquote><h1 id="让事情去发生"><a href="#让事情去发生" class="headerlink" title="让事情去发生"></a>让事情去发生</h1><p>经验增长，并不完全是一件好事，</p><p>它也许会阻止本来可能发生的事情。</p><p>我们凭着经验来进行判断，</p><p>除非遇到完全相同的初始条件，</p><p>否则，这样的判断和随意猜测并没有什么不同。</p><p>我们踩到的坑多了，走路都战战兢兢，</p><p>一年被蛇咬，十年怕井绳。</p><p>如何打破经验主义的误区呢？</p><p>那就是，让事情去发生。</p><p>现实世界中的事情都是发展中的，</p><p>我们得有胆量让事情去发展，</p><p>然后掌控它的发展过程。</p><h2 id="把责任交出去"><a href="#把责任交出去" class="headerlink" title="把责任交出去"></a>把责任交出去</h2><p>篮球是一项团体运动，</p><p>在打篮球时，我们用的最多的是传接球。</p><p>可是，新手组织后卫往往无法理解，</p><p>球传出去以后，自己都无法掌控它了，</p><p>怎样组织这次进攻呢？</p><p>这类似于其他团队合作中的责任，</p><p>不敢把责任交出去，</p><p>担心别人做不好。</p><p>这样就无法达到默契的配合。</p><p>高手组织后卫怎样理解这件事情呢？</p><p>他首先会观察比赛的发展态势，</p><p>然后通过运球去影响这个态势的走向，</p><p>随后，把球顺着态势传出去。</p><p>这样的话，其他人的行为就会被态势所牵引。</p><p>比赛仍然在掌控中。</p><p>所以，难能可贵的是，</p><p>让比赛进行，把球传出去。</p><h2 id="最佳实践"><a href="#最佳实践" class="headerlink" title="最佳实践"></a>最佳实践</h2><p>软件项目中，有前人总结的无数最佳实践，</p><p>不遵循它们，会犯很多前人已经犯过的错误。</p><p>然而，完全遵循它们，</p><p>又会抹杀掉未来的某些可能性。</p><p>每种方案都有适得其所的用处，</p><p>没有在任何情况下都不适用的方案，</p><p>不用它，其实很多人是因为没有真正掌握它。</p><p>很多方案是发展而来的，</p><p>到了那个境地，解决方案是显而易见的，</p><p>现在不用费尽心思，假定到时候很糟。</p><p>最佳实践的作用，应该是为了选择方向，</p><p>强行预测未来的细节，会担惊受怕，畏首畏尾。</p><p>你有胆量让不那么好的事情发生吗？</p><p>其实，不那么好与不那么坏，并没有什么区别。</p><h2 id="如何培养人"><a href="#如何培养人" class="headerlink" title="如何培养人"></a>如何培养人</h2><p>世界上最困难的就是把一件你很拿手的工作交给别人，</p><p>再眼睁睁看着他把事情搞砸，</p><p>而你却还能心平气和不发一言，</p><p>那是培养人。</p><p>世界上最容易的就是把一件你很拿手的工作交给别人，</p><p>再手把手地教他把事情做对，</p><p>不给他犯错机会，</p><p>那不是培养人，而是锻炼你自己。</p><p>——《有一种培养叫放手》</p><p>并不是每件事情按计划发生就是好的，</p><p>不同的发展轨迹，人们从中的收获也不同。</p><p>不让坏事情去发生，就不会身临其境，</p><p>没有把事情搞砸过，就不会反思过程。</p><h2 id="人与事情的发展"><a href="#人与事情的发展" class="headerlink" title="人与事情的发展"></a>人与事情的发展</h2><p>事情的发展，总是会涉及到很多人，</p><p>因此，个人的作用其实并不那么重要。</p><p>让事情去发展，才能观察态势，</p><p>才能改变它，才能影响它。</p><p>不要把自己看得那么重要，</p><p>包括自己在内，大家都是顺势而为，</p><p>不改变态势，结果早就被决定了，</p><p>有你没你都一样。</p><p>所以，要想赢得比赛，</p><p>就得做那个能影响比赛走势的人。</p><p>不然的话，</p><p>胜败已定，预测的再准又有什么用？</p><h2 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h2><p>Let things happen, let them win.</p><p>这不仅仅是一种洒脱的态度，</p><p>更多的是为了影响事情的整个发展过程。</p><p>如果说，新手从犯错，到学会提前意识到错误，是一个进步，</p><p>那么老手从丰富的经验，到违反经验的大胆尝试，更是一个进步。</p><p>新手，通常把方案静态的理解为当前场景的解，</p><p>而高手，更能看到事情的发展过程，</p><p>以及采取某个方案，会对事情发展产生什么影响。</p><p>让事情发生吧，我来做观众。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;有关10000小时定律&quot;&gt;&lt;a href=&quot;#有关10000小时定律&quot; class=&quot;headerlink&quot; title=&quot;有关10000小时定律&quot;&gt;&lt;/a&gt;有关10000小时定律&lt;/h1&gt;&lt;h2 id=&quot;引言&quot;&gt;&lt;a href=&quot;#引言&quot; class=&quot;head</summary>
      
    
    
    
    
    <category term="感悟" scheme="https://xtaiyang.github.io/tags/%E6%84%9F%E6%82%9F/"/>
    
  </entry>
  
  <entry>
    <title>智能对话系统与知识融合技术探析</title>
    <link href="https://xtaiyang.github.io/posts/172b526.html"/>
    <id>https://xtaiyang.github.io/posts/172b526.html</id>
    <published>2024-09-30T16:00:00.000Z</published>
    <updated>2025-03-11T08:14:12.499Z</updated>
    
    <content type="html"><![CDATA[<h1 id="智能对话系统与知识融合技术探析"><a href="#智能对话系统与知识融合技术探析" class="headerlink" title="智能对话系统与知识融合技术探析"></a>智能对话系统与知识融合技术探析</h1><p>在人工智能快速发展的今天，智能对话系统和知识融合技术正在改变着我们与数据交互的方式。本文将深入探讨几种主流的对话系统方案，以及如何通过知识图谱实现多模态数据的智能融合。</p><h2 id="一、智能对话系统方案对比"><a href="#一、智能对话系统方案对比" class="headerlink" title="一、智能对话系统方案对比"></a>一、智能对话系统方案对比</h2><img src="/posts/172b526/830bafa743a98e0beefbe98a512a4676.png" class="" title="图片描述"><!-- ![830bafa743a98e0beefbe98a512a4676.png](https://img.picui.cn/free/2024/11/11/6731c2e8d6237.png) --><h3 id="1-NL2SQL：简单高效的查询助手"><a href="#1-NL2SQL：简单高效的查询助手" class="headerlink" title="1. NL2SQL：简单高效的查询助手"></a>1. NL2SQL：简单高效的查询助手</h3><p>NL2SQL就像一位敏捷的数据服务员，能够快速将自然语言转换为数据库查询语句。</p><p><strong>优势：</strong></p><ul><li>操作简单直观</li><li>响应迅速</li><li>适合日常查询任务</li></ul><p><strong>局限性：</strong></p><ul><li>难以处理复杂分析需求</li><li>无法识别数据间深层关系</li><li>在处理复杂SQL时可能出现性能瓶颈</li><li>存在潜在的安全隐患</li></ul><h3 id="2-DSL：专业的领域对话专家"><a href="#2-DSL：专业的领域对话专家" class="headerlink" title="2. DSL：专业的领域对话专家"></a>2. DSL：专业的领域对话专家</h3><p>DSL（Domain-Specific Language）是为特定领域设计的专门语言，就像各个领域的专家顾问。</p><p>它是一种为特定目的或特定领域设计的编程语言，与通用编程语言相比，DSL更加专注于解决特定类型的问题，因此其语法和词汇更加简洁和高效。</p><p><strong>特点：</strong></p><ul><li>语法和词汇更加简洁高效</li><li>专注于解决特定领域问题</li><li>强调声明性而非执行细节</li><li>将意图与实现分离</li></ul><p><strong>应用举例：</strong></p><ul><li>正则表达式：用于文本处理的DSL</li><li>配置文件：如Nginx配置，用于服务器行为定制</li></ul><p><strong>挑战：</strong></p><ul><li>需要大量训练和维护成本</li><li>开发周期较长</li><li>需要专业知识支持</li></ul><h3 id="3-指标库-ChatBI：预制菜单式对话"><a href="#3-指标库-ChatBI：预制菜单式对话" class="headerlink" title="3. 指标库+ChatBI：预制菜单式对话"></a>3. 指标库+ChatBI：预制菜单式对话</h3><p>这种方案像是一份详尽的数据分析菜单，用户可以直接选择所需指标。</p><p><strong>优势：</strong></p><ul><li>准确性高</li><li>权限管理便捷</li><li>响应速度快</li></ul><p><strong>局限性：</strong></p><ul><li>灵活性不足</li><li>难以处理预设指标之外的需求</li><li>动态分析能力受限</li></ul><hr><p>领域特定语言（DSL）的特点和价值：</p><ol><li><strong>内部DSL与普通代码的区别</strong>：<ul><li>内部DSL的代码更倾向于声明性质，而不是执行动作。这意味着内部DSL的代码更关注于“做什么”（What），而不是“怎么做”（How）。这种声明性质的代码强调的是意图，而不是具体的实现细节。</li></ul></li><li><strong>抽象级别的差异</strong>：<ul><li>“怎么做”涉及到具体的实现，是一种较低层次的抽象，关注的是技术细节和执行步骤。</li><li>“做什么”则是一种更高层次的抽象，关注的是目标和意图，而不是如何达成这些目标。</li></ul></li><li><strong>意图与实现的分离</strong>：<ul><li>内部DSL的一个关键特点是将意图（做什么）与实现（怎么做）分离。这种分离是内部DSL与普通程序代码的一个重要区别，也是良好设计的一个考虑因素。</li></ul></li><li><strong>DSL的关键元素</strong>：<ul><li>四个关键元素：计算机程序设计语言、语言性、受限的表达性和针对领域。</li><li>其中，语言性强调DSL需要有连贯的表达能力，即能够清晰地表达出使用者的意图。</li></ul></li><li><strong>代码的表达能力</strong>：<ul><li>程序员在编写代码时应该关注代码的表达能力，这是区分优秀程序员和普通程序员的一个因素。</li><li>普通程序员可能只关注功能的实现，而优秀的程序员会将代码的不同层次分离，将意图和实现分开，使得实现可以被替换。</li></ul></li><li><strong>学习内部DSL的价值</strong>：<ul><li>学习内部DSL的价值在于，即使不设计一个完整的内部DSL，学会将意图与实现分离也是对日常编码工作非常有价值的。</li></ul></li></ol><hr><p>领域特定语言（DSL）的两种类型：外部DSL和内部DSL。</p><ol><li><strong>外部DSL</strong>：这种DSL是完全独立的语言，有自己的语法和语义。它不依赖于任何宿主语言，可以独立运行。外部DSL通常需要更多的开发工作，因为它需要从头开始构建整个语言的语法、解析器、编译器等。</li><li><strong>内部DSL</strong>：这种DSL是嵌入在宿主语言（如Java、Python等）中的，它使用宿主语言的语法和语义，但专注于特定领域的表达。内部DSL的开发成本相对较低，因为它可以利用宿主语言的基础设施，如编译器、开发工具等。</li></ol><p>内部DSL相对于外部DSL的优势：</p><ul><li><strong>开发成本更低</strong>：因为内部DSL可以利用宿主语言的现有资源，所以开发和维护起来更加经济。</li><li><strong>与日常工作结合得更加紧密</strong>：由于内部DSL使用宿主语言的语法，对于已经熟悉宿主语言的开发者来说，学习和使用内部DSL更加容易，可以更自然地融入到日常工作流程中。</li></ul><hr><h2 id="二、知识图谱增强与多模态融合"><a href="#二、知识图谱增强与多模态融合" class="headerlink" title="二、知识图谱增强与多模态融合"></a>二、知识图谱增强与多模态融合</h2><p><strong>知识图谱（Knowledge Graph）</strong></p><ul><li><strong>定义</strong>：知识图谱是一种结构化的语义知识库，它通过图的形式存储实体（节点）和它们之间的关系（边）。这种图结构使得知识图谱能够表示复杂的关系和属性，便于进行知识推理和查询。</li><li><strong>应用</strong>：知识图谱常用于提供实体识别、关系抽取、语义搜索等功能。在图像识别中，知识图谱可以用来识别图像中的实体，并将其与已知的信息关联起来。</li></ul><p><strong>知识库（Knowledge Base）</strong></p><ul><li><strong>定义</strong>：知识库是存储知识的集合，它可以是结构化的（如数据库）或半结构化的（如文件系统）。知识库可以包含各种类型的数据，如文本、图像、视频等。</li><li><strong>应用</strong>：知识库可以用于支持决策、提供背景信息、辅助数据分析等。在多模态数据融合中，知识库可以提供不同模态数据之间的关联信息。</li></ul><h3 id="1-知识图谱在Text-to-SQL中的应用"><a href="#1-知识图谱在Text-to-SQL中的应用" class="headerlink" title="1. 知识图谱在Text-to-SQL中的应用"></a>1. 知识图谱在Text-to-SQL中的应用</h3><p><strong>优势：</strong></p><ul><li><strong>增强语义理解能力</strong>：知识图谱通过实体和关系的结构化表示，可以提供更丰富的语义信息，帮助模型更好地理解查询中的实体和它们之间的关系。</li><li><strong>提升模型泛化性</strong>：知识图谱可以提供跨领域的知识，有助于模型在不同数据库和领域之间进行泛化。</li><li><strong>改善实体链接精确度</strong>：在自然语言查询中，用户可能会提到一些特定的实体，知识图谱可以帮助模型将这些实体与数据库中的相应条目进行链接。</li><li><strong>支持复杂查询推理</strong>：知识图谱中的关系可以用于推理，帮助模型生成更复杂的查询，例如那些需要多跳推理的查询。</li><li><strong>提高系统鲁棒性</strong>：知识图谱可以减少对训练数据的依赖，提高模型在面对未见过的查询或数据库结构时的鲁棒性。</li><li><strong>支持多模态信息融合</strong>：知识图谱可以整合来自不同来源的信息，包括文本、图像等，为Text-to-SQL任务提供更全面的上下文。</li></ul><p><strong>挑战：</strong></p><ul><li><strong>构建和维护成本高</strong>：高质量的知识图谱需要大量的工作来构建和维护。</li><li><strong>动态信息更新困难</strong>：知识图谱可能难以及时更新以反映数据库中的最新变化。</li><li><strong>系统复杂度增加</strong>：集成知识图谱可能会增加系统的复杂性，需要额外的算法来处理图结构数据。</li></ul><h3 id="2-多模态数据融合技术"><a href="#2-多模态数据融合技术" class="headerlink" title="2. 多模态数据融合技术"></a>2. 多模态数据融合技术</h3><h4 id="1-图像识别"><a href="#1-图像识别" class="headerlink" title="1. 图像识别"></a>1. 图像识别</h4><p><strong>目的</strong>：识别图像中的对象、场景或活动。</p><p><strong>应用实例</strong>：通过知识图谱提供的信息，识别“这幅画是谁画的？”</p><ul><li>工作流程<ol><li><strong>图像处理</strong>：使用计算机视觉技术（如卷积神经网络CNN）来识别图像中的视觉特征。</li><li><strong>特征提取</strong>：从图像中提取关键特征，如颜色、形状、纹理等。</li><li><strong>知识图谱匹配</strong>：将提取的特征与知识图谱中的实体进行匹配，以确定图像中的对象或场景。</li><li><strong>结果输出</strong>：输出识别结果，例如“这幅画是梵高画的”。</li></ol></li></ul><h4 id="2-语音识别"><a href="#2-语音识别" class="headerlink" title="2. 语音识别"></a>2. 语音识别</h4><p><strong>目的</strong>：将语音信号转换为文本数据。</p><p><strong>应用实例</strong>：通过语音识别查询“我要查询天气预报”。</p><ul><li>工作流程<ol><li><strong>语音采集</strong>：通过麦克风等设备采集语音信号。</li><li><strong>语音预处理</strong>：对语音信号进行降噪、分割等处理。</li><li><strong>特征提取</strong>：提取语音特征，如梅尔频率倒谱系数（MFCC）。</li><li><strong>模型识别</strong>：使用深度学习模型（如循环神经网络RNN）将语音特征转换为文本。</li><li><strong>结果输出</strong>：输出识别的文本，例如“我要查询天气预报”。</li></ol></li></ul><h4 id="3-情感分析"><a href="#3-情感分析" class="headerlink" title="3. 情感分析"></a>3. 情感分析</h4><p><strong>目的</strong>：分析文本中的情感倾向，如正面、负面或中性。</p><p><strong>应用实例</strong>：判断“这篇文章的评价是正面的还是负面的？”</p><ul><li>工作流程<ol><li><strong>文本预处理</strong>：对文本进行分词、去除停用词等处理。</li><li><strong>特征提取</strong>：提取文本特征，如词袋模型、TF-IDF等。</li><li><strong>情感分类</strong>：使用机器学习或深度学习模型对情感进行分类。</li><li><strong>结果输出</strong>：输出情感分析结果，例如“这篇文章的评价是正面的”。</li></ol></li></ul><h3 id="3-知识库与知识图谱的协同"><a href="#3-知识库与知识图谱的协同" class="headerlink" title="3. 知识库与知识图谱的协同"></a>3. 知识库与知识图谱的协同</h3><p><strong>知识图谱特点：</strong></p><ul><li>结构化的语义知识表示</li><li>支持实体关系推理</li><li>便于知识查询和检索</li></ul><p><strong>知识库功能：</strong></p><ul><li>支持多种数据类型存储</li><li>提供丰富的背景信息</li><li>辅助决策分析</li></ul><p><strong>步骤：</strong></p><ol><li><strong>知识库构建</strong>：构建包含实体和关系的大规模知识库。</li><li><strong>数据采集</strong>：采集多模态数据，如图像、文本等。</li><li><strong>特征提取</strong>：从多模态数据中提取特征。</li><li><strong>匹配</strong>：<ul><li><strong>知识图谱匹配</strong>：将提取的特征与知识图谱中的实体进行匹配，以确定图像中的对象或场景。例如，通过图像识别技术提取的特征与知识图谱中的实体（如“梵高”）进行匹配，以识别出图像中的画家。</li><li><strong>知识库匹配</strong>：将提取的特征与知识库中的信息进行匹配，以整合不同模态的数据。例如，将语音识别结果与知识库中的天气信息进行匹配，以提供天气预报服务。</li></ul></li><li><strong>数据整合</strong>：将匹配后的信息整合在一起，形成更全面的知识表示。</li><li><strong>知识推理</strong>：利用知识库中的信息进行推理，以提供更全面的理解。</li><li><strong>结果输出</strong>：输出融合后的结果，例如“这幅画是梵高画的，画中的场景是星空”。</li></ol><h2 id="三、未来展望"><a href="#三、未来展望" class="headerlink" title="三、未来展望"></a>三、未来展望</h2><p>随着技术的不断发展，我们可以期待：</p><ol><li>更智能的对话系统，能够理解更复杂的用户意图</li><li>更高效的知识融合方案，实现真正的多模态智能交互</li><li>更完善的知识图谱生态，支持更广泛的应用场景</li></ol><p>智能对话系统和知识融合技术的发展将继续推动人工智能向着更加智能和实用的方向发展，为用户提供更优质的智能服务体验。</p><h2 id="参考架构图"><a href="#参考架构图" class="headerlink" title="参考架构图"></a>参考架构图</h2><img src="/posts/172b526/fd035eb018fc64702f4482c74f4bb10b520c0d5d.png" class="" title="图片描述"><img src="/posts/172b526/62873a531e111a5146a937b1d3383d0181c26bf3.png" class="" title="图片描述">]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;智能对话系统与知识融合技术探析&quot;&gt;&lt;a href=&quot;#智能对话系统与知识融合技术探析&quot; class=&quot;headerlink&quot; title=&quot;智能对话系统与知识融合技术探析&quot;&gt;&lt;/a&gt;智能对话系统与知识融合技术探析&lt;/h1&gt;&lt;p&gt;在人工智能快速发展的今天，智能对话系</summary>
      
    
    
    
    
    <category term="AI" scheme="https://xtaiyang.github.io/tags/AI/"/>
    
    <category term="NLP" scheme="https://xtaiyang.github.io/tags/NLP/"/>
    
  </entry>
  
</feed>
